[
  {
    "path": "posts/faviconplease/",
    "title": "faviconPlease",
    "description": "A favicon-grabber for R",
    "author": [
      {
        "name": "John Blischak",
        "url": "https://jdblischak.com"
      }
    ],
    "date": "2022-03-25",
    "categories": [
      "Software Development with R"
    ],
    "contents": "\ntl;dr The R package faviconPlease returns the URL to the favicon file for any website. Also available to install from CRAN\nJenny Slate wants her favicon. 1Motivation\nFor the OmicNavigator web app I helped develop (see my previous blog post), we allow the user to include external linkouts for more information on the features (see example screenshot below), e.g. a link to the Ensembl page for each gene. In order to make the display appealing and intuitive, we wanted to display the links using each website’s unique favicon. Since we don’t know in advance which external resources a user will include, we needed a general solution that could obtain a favicon for any website.\nThe first 2 columns of the interactive table contain favicons which are linkouts to external websitesMy initial plan was to use a 3rd party service to obtain the favicon to display. Search engine providers like Google and DuckDuckGo display the favicon for each site in their search results, and they conveniently provide an API for others to query their favicon databases. Frustratingly though, the coverage was not as good as I had anticipated, especially for the scientific websites our users were likely to need. And the availability changes over time. Some examples from back when I was originally writing the package: 1) DuckDuckGo had the favicon for GitHub but not Google, and 2) Google had the favicon for AmiGO but not DuckDuckGo. The last time I checked though both services provide favicons for GitHub and AmiGO.\nGitHub’s favicon\nhttps://icons.duckduckgo.com/ip3/github.com.ico \nhttps://www.google.com/s2/favicons?domain_url=github.com \n\nAmiGo’s favicon\nhttps://www.google.com/s2/favicons?domain_url=amigo.geneontology.org \nhttps://icons.duckduckgo.com/ip3/amigo.geneontology.org.ico \n\nFortunately though they both provide a generic favicon to insert when they don’t have the favicon:\nDuckDuckGo \nGoogle \nAnother big limitation is that the search engine providers only have favicons available for publicly accessible websites. If you have internal websites with their own favicons, this requires a more involved approach.\nNext I looked for available open source favicon-grabber software. As expected, these are mainly written in more web-oriented programming languages like JavaScript and PHP.\nGrabbing the favicon mainly involves parsing HTML and/or downloading files, which R can handle, so I decided to create a favicon-grabber for R: faviconPlease!\nHow favicons work\nFrom the mdn web docs:\n\nA favicon (favorite icon) is a tiny icon included along with a website, which is displayed in places like the browser’s address bar, page tabs and bookmarks menu.\n\nThe default option for a website to display a favicon in a web browser tab is to provide the file favicon.ico at the root of the web server. If every web site used this option, then finding favicons would be trivial. Unfortunately there are many ways to configure a favicon.\nWhen a website doesn’t provide favicon.ico, they often specify the favicon file with a <link> element in the <head> of the file. The rel attribute is set to either \"icon\" or \"shortcut icon\".\n<link rel=\"icon\" href=\"path/to/favicon.ico\">\n\n<link rel=\"shortcut icon\" href=\"path/to/favicon.ico\">\nThis also isn’t too bad. You can parse the HTML and then query with XPath to obtain the URL.\nA big challenge is that the href attribute with the path to the file can be specified in so many different ways:\n<!--  absolute: the full URL to the file -->\n<link rel=\"icon\" href=\"https://example.com/path/to/favicon.ico\">\n\n<!--  protocol-relative - everything but the http(s) -->\n<link rel=\"icon\" href=\"//othersite.com/path/to/favicon.ico\">\n\n<!--  root-relative - relative to the root of the server, as opposed to the current file -->\n<link rel=\"icon\" href=\"/path/to/favicon.ico\">\n\n<!--  relative - relative to the current file -->\n<link rel=\"icon\" href=\"path/to/favicon.ico\">\nAnd an added twist is that relative paths can be globally modified by the base element:\n\nThe  HTML element specifies the base URL to use for all relative URLs in a document. There can be only one  element in a document.\n\nImplementation\nMy goal was to create a single function that abstracted away all of the above complexity. You pass a URL, and it returns a URL to a favicon you can use to display in your app. It’ll try various strategies to obtain a favicon, and fall back to one of the default favicons from DuckDuckGo or Google. Here’s an example:\n\n\nlibrary(faviconPlease)\nfaviconPlease(\"https://github.com/\")\n\n\n[1] \"https://github.githubassets.com/favicons/favicon.svg\"\n\nBy default, faviconPlease() uses the following strategy to find the URL to the favicon for a given website. It stops once it finds a URL and returns it.\nDownload the HTML file and search its <head> for any <link> elements with rel=\"icon\" or rel=\"shortcut icon\".\nDownload the HTML file at the root of the server (i.e. discard the path) and search its <head> for any <link> elements with rel=\"icon\" or rel=\"shortcut icon\".\nAttempt to download a file called favicon.ico at the root of the server. This is the default location that a browser looks if the HTML file does not specify an alternative location in a <link> element. If the file favicon.ico is successfully downloaded, then this URL is returned.\nIf the above steps fail, as a fallback, use the favicon service provided by the search engine DuckDuckGo, e.g. https://icons.duckduckgo.com/ip3/github.com.ico. This provides a nice default for websites that don’t have a favicon (or can’t be easily found).\nI also wanted to make sure the function was extensible and customizable so that it could be useful to others without me having to implement every alternative strategy for finding favicons. To accomplish this, I took advantage of the feature that R has first-class functions. The argument functions accepts a list of functions to find a favicon URL. And the argument fallback accepts a single function that always returns a URL (e.g. from DuckDuckGo).\n\n\nargs(faviconPlease)\n\n\nfunction (links, functions = list(faviconLink, faviconIco), fallback = faviconDuckDuckGo) \nNULL\n\nThis way, if you instead wanted to first check for the existence of favicon.ico and use Google as a fallback, you could adjust the functions passed to faviconPlease. In the case of github.com, it does find a favicon.ico file.\n\n\nlibrary(faviconPlease)\nfaviconPlease(\"https://github.com/\", functions = list(faviconIco, faviconLink),\n              fallback = faviconGoogle)\n\n\n[1] \"https://github.com/favicon.ico\"\n\nFurthermore, if you wanted to create your own favicon search function, you could model it after faviconLink() and faviconIco(), and then pass it in the list of functions.\nChallenges\nThe first challenge is that download.file() is not fool-proof. I first encountered this issue when trying to write workshop tutorials. A simple call to download.file() will fail in many different situations depending on the operating system. ?download.file recommends:\n\nSetting the method should be left to the end user.\n\nAnd thus this is what I did, by exposing the argument method (and others) to pass from faviconIco() to download.file(). Fortunately it is also possible to control this behavior with a global option \"download.file.method\".\nA second big challenge is security policies. As a primary example, Ubuntu 20.04 increased the default security restrictions when accessing web resources, and many websites haven’t updated their SSL certificates. This is one way the ability to create custom functions is useful. To use less-restrictive security policies, you can wrap the call to faviconIco() in a new function and specifically pass the flags --no-check-certificate and --ciphers=DEFAULT:@SECLEVEL=1 directly to wget.\n\n\nfaviconIcoUbuntu20 <- function(scheme, server, path) {\n  faviconIco(scheme, server, path, method = \"wget\",\n             extra = c(\"--no-check-certificate\",\n                       \"--ciphers=DEFAULT:@SECLEVEL=1\"))\n}\n\n\n\nLastly, it’s a challenge to anticipate all the different ways a website may serve their favicon. If you find any websites that have a favicon but that faviconPlease() fails to find, please open an Issue so that I can potentially expand the support for more sites.\n\nI downloaded the image from giphy.com and edited it with ezgif.com. I obviously have no rights to this image, so i guess “fair use”??↩\n",
    "preview": "posts/faviconplease/img/faviconPlease.gif",
    "last_modified": "2023-01-18T16:46:24+00:00",
    "input_file": {}
  },
  {
    "path": "posts/omicnavigator/",
    "title": "OmicNavigator",
    "description": "Open-source software for omic data analysis and visualization",
    "author": [
      {
        "name": "John Blischak",
        "url": "https://jdblischak.com"
      }
    ],
    "date": "2021-12-03",
    "categories": [
      "bioinformatics"
    ],
    "contents": "\n\nContents\nOverview: Facilitating collaborative omics research\nApp features\nCreate an OmicNavigator study with your own data\nArchitecture\nComparison to other dashboard-like tools\nAcknowledgements\n\ntl;dr We built an interactive web app for exploring the results of genomics assays: OmicNavigator. You input your data with R, and then OmicNavigator instantly creates a dynamic web app for you to use and share with your collaborators! Available from CRAN and GitHub.\nOverview: Facilitating collaborative omics research\nThe motivation to build OmicNavigator arose from the desire to improve research projects at AbbVie that involved high-throughput omics assays. These are highly collaborative projects that require expertise of both disease specialists and bioinformaticians. The disease specialists need to explore the results in-depth, but they are often dependent on the bioinformaticians to query and visualize the data. And the bioinformaticians’ time is best spent on performing statistical analyses and generating custom visualizations. Any time spent generating generic boxplots or yet another basic Shiny app is time they could have spent on more high-value activities. OmicNavigator to the rescue! OmicNavigator aims to facilitate these routine interactions and streamline the research project for all participants:\nDisease specialist: Instead of emailing your collaborator each time you want to explore a new gene of interest, you can open your browser and explore all the study results whenever you want. You can also perform your own filters to subset the data to the genes you think are the most interesting, and export the plots to include in your presentations.\nBioinformatician: The next time you perform a differential expression and enrichment analysis, you can send your collaborators to your OmicNavigator web app to explore the results. You don’t have to become a web dev expert. You can instead spend your time creating custom visualizations, which you can then integrate into OmicNavigator, and your collaborators will be able to auto-generate your plots for whichever genes they dynamically select in the app.\nManager: OmicNavigator is able to display the results of many studies from the same interface. You just have to choose which study to explore in the drop down menu. Instead of having the results from each project scattered in different locations, you can have them all in one place.\nData curator: The data in OmicNavigator studies are organized in a uniform way. Thus instead of having to manually curate each project’s data, you can write scripts to automatically convert OmicNavigator data files into the format you need.\nWeb developer: If you’re a web developer that needs to integrate results stored in OmicNavigator into your application, you don’t need to worry about learning R or waiting for the bioinformatician to package the data into the format you need. The data in OmicNavigator studies is accessible from a standard API and exportable in many formats, e.g. JSON (more on that below in the section Architecture). You can have your app query the OmicNavigator API to always automatically download the latest data.\nApp features\nThe app has two main panes: one for differential expression results and one for enrichment results.\nThe differential expression pane includes:\na dynamic results table that you can search, filter, sort, etc.\nan interactive scatter plot. If you drag and select points in the plot, the table is automatically filtered to only include these features. A typical use case is creating a “volcano”-style plot, but you can compare any two numeric columns that you like\nCustom R plots. If the bioinformatician has added any custom plots, you can select any gene to visualize it (and export the plot if needed)\nA filter to subset genes based on the results from all the available tests. This makes it easier to e.g. find genes that are significantly differentially expressed in one test but not others\nScreenshot of differential expression pane (R package version 1.8.0 and web app version 1.4.0)The enrichment analysis pane includes:\na dynamic table with the enrichment results that you can search, filter, sort, etc.\na network view for visualizing the relationships between the various terms used for enrichment (e.g. how many genes are in common between the terms)\na “barcode” view for investigating the genes driving the enrichment of a term in an interactive barcode plot\nA filter to subset genes based on the enrichment results from all the available tests. This makes it is easier to e.g. find terms that are significantly enriched in one test but not others\nScreenshot of enrichment analysis pane (R package version 1.8.0 and web app version 1.4.0)There are even more existing features, e.g. custom plots for visualizing multiple features at once, and more features in development, but the above should give you a good sense of the app’s capabilities.\nCreate an OmicNavigator study with your own data\nAfter performing the statistical analysis, the bioinformatician deposits the data into an OmicNavigator study. This should require minimal data manipulation since OmicNavigator provides helper functions to ingest data frames directly in R. Once assembled, OmicNavigator exports the data as an R package for consumption by the web app.\nAnd you don’t need to include every single piece of potential data. The app only enables the features that are supported based on the available data. For example, if you didn’t perform an enrichment analysis, you can still use OmicNavigator to share differential expression results.\nBelow is a quick start guide to give you an idea of how this works. To learn more, check out the extensive User’s Guide and example study.\n\n\n# Install and load the package\ninstall.packages(\"OmicNavigator\", dependencies = TRUE)\nlibrary(OmicNavigator)\n\n# Create a very minimal study with a single results table\nquickstart <- createStudy(\"quickstart\")\ndata(\"RNAseq123\")\nhead(basal.vs.lp)\nresultsTable <- basal.vs.lp[, -2:-3]\nquickstart <- addResults(quickstart, list(model = list(test = resultsTable)))\ninstallStudy(quickstart)\n\n# (optional) Install the example study RNAseq123 which demos many of the app's\n# available features\ninstall.packages(c(\"gplots\", \"viridis\"))\ntarball <- \"https://github.com/abbvie-external/OmicNavigatorExample/releases/latest/download/ONstudyRNAseq123.tar.gz\"\ninstall.packages(tarball, repos = NULL)\n\n# Install and start the web app\ninstallApp()\nstartApp()\n\n\n\nArchitecture\nThe central piece of infrastructure is OpenCPU, an amazing framework for creating web apps with R. The basic idea behind OpenCPU is that is exposes all the functions in the R packages installed on the machine via a uniform HTTP API. This makes it possible for web apps to harness the power of R without R developers having to perform extra steps to make their code deployable.\nThis is an incredibly powerful paradigm. It lets me as the R developer use all the tools I am familiar with. My code is organized as an R package, my functions are documented, and I’m able to easily test my functions since they return standard R objects like lists and data frames. Then my collaborators developing the front-end JavaScript app are able to call my R functions via HTTP and request JSON formatted data in return. They don’t have to worry about R-specific things like data frames and factors; in fact, they don’t even need to install R! They are able to focus on the front-end using whichever tools they prefer, in this case React and D3.\nAnd because OpenCPU is serving the R functions via a standard HTTP API, other developers can query and access the data for inclusion in other apps throughout the organization. By default OpenCPU provides many options for output formats, including JSON, NDJSON, and protocol buffers.\nIn summary, OpenCPU allows us to take advantage of the best of the R and JavaScript ecosystems.\nComparison to other dashboard-like tools\nThere are lots of options for creating dashboards. OmicNavigator aims to complement these existing options. OmicNavigator provides dynamic web app capabilities to the R user while also allowing them to customize the app with bespoke plotting functions.\nTo best illustrate how OmicNavigator fits in, below I compare it to some other available options:\nPoint-and-click GUIs: While the app itself is point-and-click, it is created and deployed by the R user. This allows them to easily update their study. When new data arrives, or they add a new statistical model, simply update the data-in script and re-export the OmicNavigator study\nDashboard frameworks: Frameworks like Shiny and streamlit enable you to quickly create a custom dashboard from scratch using R and Python, respectively. This is super useful, but can also be time-consuming. OmicNavigator attempts to cover the common use cases of reporting differential expression results so that the bioinformatician can spend their time developing bespoke dashboards for the unique aspects of a particular study\nAcknowledgements\nOmicNavigator is the brain child of Brett Engelmann. The web app has been primarily developed by Paul Nordlund, Terry Ernst, and Joe Dalen. Additional contributors include Justin Moore, Akshay Bhamidipati , and Marco Curado.\nAnd of course thanks to AbbVie for supporting the development of this open source tool!\n\n\n\n",
    "preview": "https://raw.githubusercontent.com/abbvie-external/OmicNavigator/main/man/figures/omicnavigator-dark-text.png",
    "last_modified": "2023-01-18T16:46:24+00:00",
    "input_file": {}
  },
  {
    "path": "posts/chircollab2020/",
    "title": "My experience at the Chicago R Collaborative 2020",
    "description": "A fun event to build the R community",
    "author": [
      {
        "name": "John Blischak",
        "url": "https://jdblischak.com"
      }
    ],
    "date": "2020-05-10",
    "categories": [
      "conference"
    ],
    "contents": "\nTable of Contents\nCollaborating with Git and GitHub\nContributions to workflowr\nLinks to other chircollab 2020 projects and posts\nOne of my favorite parts of R is the community, so I was excited for this year’s Chicago R Collaborative (rebranded from last year’s Chicago R Unconference). I was disappointed that the in-person event scheduled for March had to be canceled, but fortunately the amazing organizers (Angela Li, Emily Riederer, Sydney Purdue, Will Bonnell) were able to quickly pivot to hosting a virtual event last April.\nAs a mentor for the event, my main goal was to empower R users to become R developers by contributing to open source R projects. I did this via live tutorials demonstrating how to use Git and GitHub, and mentoring contributions to my R package workflowr.\nCollaborating with Git and GitHub\nA big hurdle for new contributors is learning the technical and social skills to contribute via Git and GitHub. To help with this, I teamed up with Mauro Lepore to do a live demonstration of contributing to an open source project on GitHub. Mauro created a new Git repository, I made a contribution, and he merged my Pull Request. We didn’t record the actual demo so that attendees wouldn’t be self-conscious about asking questions. However, you can watch the practice demo we recorded prior to the event:\n\n\nAfter our demo, we had the attendees create their own Pull Requests to this new repository. They submitted R-related tips and tricks. Check out the repository rcollab-git to read their advice. We had 4 attendees successfully submit a Pull Request: Jim Gruman, Eric Nantz, Anna Vasylytsya, and Stefany Samp.\nIf you’re interested in practicing submitting a Pull Request on GitHub, check out the tutorial I co-authored, A Quick Introduction to Version Control with Git and GitHub. It explains how to submit a Pull Request to the accompanying repository git-for-science.\nThe second live demo I did was an impromptu idea from Anna Vasylytsya. James Lamb had submitted a Pull Request to my workflowr package, and Anna suggested that I do the code review live so that others could see what this process was like. This was a great learning experience, especially for me!\nI reviewed the Pull Request and submitted a GitHub review. I explained that one of the reasons I liked using the GitHub review instead of leaving individual comments was so that the person that submitted the Pull Request would receive one notification instead of individual notifications for each comment (I’ve been on the other end of this, and it can be demoralizing, because the reviewer often starts with the requested changes, and doesn’t add the positive feedback until the end). James added his strategy for leaving reviews, which I thought was a great summary of a social practice that I had adopted but didn’t realize: If you are responsible for the all of the code that the Pull Request affects, you should leave a review to approve or request changes. On the other hand, if you just want to lend your expertise on one aspect of the Pull Request, leave single comments without giving a review.\nI also learned more from James about the GitHub feature to suggest changes on a Pull Request. I knew that GitHub had made it possible to suggest edits directly in the source code, but I didn’t realize that the submitter of the Pull Request can accept and convert the suggested edit into a commit without having to leave the GitHub user interface. Very slick.\nAfter having done these two live demos, I realized too late that some aspects of my GitHub user interface may have looked different than the standard because I have the browser extension refined-github installed. In retrospect I should have disabled it for the demos to remove any unnecessary distractions. However, on the plus side it did give me the opportunity to share this GitHub tip. If you spend a lot of time working on GitHub, I recommend giving it a try (especially if you use a lot of CI services). The extension is available for Firefox and Chrome.\nContributions to workflowr\nI was very happy to have 3 new collaborators contribute to my R package workflowr. Their contributions described below are already available in the latest CRAN release (version 1.6.2).\nAnh Tran overhauled workflowr’s RStudio project template (workflowr Issue #193). The existing version I had created was very minimal, and only exposed a few of the arguments to wflow_start(). Anh created a wrapper function to handle additional arguments passed from RStudio and updated the user interface (PR #200). Furthermore, he created unit tests for the new wrapper function! (PR #205)\nThe RStudio GUI menu for creating a new workflowr project: before and afterSydney Purdue added some extra error handling to wflow_start() to fail early when the input argument overwrite is used incorrectly (workflowr Issue #194). And she wrote her first unit test! (PR #202)\nJames Lamb updated the HTTP calls to the GitHub API in wflow_use_github() to be more resilient to transient network issues by using httr::RETRY(), which automatically retries HTTP calls more than once (PR #199, PR #201).\nLinks to other chircollab 2020 projects and posts\nProjects:\nkableExtra cookbook - Sharla Gelfand, Brooke Anderson, and Jim Gruman\nCollaborative introduction slides converted to xaringan - Eric Nantz and Angela Li\ngooglecivic - Will Bonnell, Edward Visel, and Natalia Block\nhttr::RETRY() - James Lamb, Anna Vasylytsya, Adam Austin, and Tomas Okal\ndatos - Mauro Lepore and Jonathan Keane\ntakehomecarpentries - Gabi Cipriano\nAll proposed projects\nBlog posts:\nNatalia Block - R Collaborative Conference: tips to make the most of it\nJim Gruman - Chicago R Collaborative 2020\nTweets: #chircollab\n\n\n",
    "preview": "https://github.com/chircollab/chircollab20/raw/master/img/logo.png",
    "last_modified": "2023-01-18T16:46:24+00:00",
    "input_file": {}
  },
  {
    "path": "posts/wsl-r/",
    "title": "R programming in the Windows Subsystem for Linux",
    "description": "A comprehensive guide to setup an R development environment for Ubuntu running\nin the Windows Subsystem for Linux (WSL)",
    "author": [
      {
        "name": "John Blischak",
        "url": "https://jdblischak.com"
      }
    ],
    "date": "2020-03-31",
    "categories": [
      "Software Development with R",
      "Ubuntu",
      "Windows"
    ],
    "contents": "\n\nContents\nIntroduction\nInstall Windows Subsytem for Linux from the Windows Store\nInstall Windows Subsytem for Linux from PowerShell\nInstall software with APT\nInstall R\nInstall R packages\nInstall Bioconductor packages\nInstall OpenCPU server\nSSH keys\nAccess Windows filesystem\nVS Code remote extension\nRun Bash terminal from RStudio\nCopy and paste to and from terminal\nWSL2\nConclusions\nOther resources\n\nUpdates (2022-11-04)\nThis post is now a few years old, thus some of the specifics are outdated. It\ninstalls Ubuntu Bionic (18.04) on Windows via WSL1. The general strategy below\nis still valid, but I recommend you install a more recent version of Ubuntu via\nWSL2 (which is now the default)\nIf you previously installed Ubuntu via WSL1, you can upgrade the existing\ninstallation to WSL2\nThis post describes how to run RStudio Server in the WSL and access it from\nyour browser running in Windows. However, Tom\nPalmer taught me about an alternative option. You\ncan install a graphical desktop environment in WSL and then run RStudio Desktop\ndirectly on Ubuntu. This requires some extra setup, but if you want a more\nintegrated Ubuntu experience, the effort will be worth it. See Tom’s\nIssue for more\ndetails\nIntroduction\nThe Windows Subsystem for Linux (WSL) is really impressive. It provides a\nconvenient and smooth experience for running Ubuntu (and other Linux\ndistributions) from Windows because the Linux executables are able to run\nnatively on the Windows machine. In the past, I’ve tried running Ubuntu in a\nvirtual machine, but it was\npainfully slow, so much so that I never really used it for real work.1\nI’ve also dual-booted multiple machines, but I always found reformatting the\nhard drive to be a nerve-wracking and time-consuming experience. And while I\nproductively used a dual-boot setup for years, I essentially only used Ubuntu\nduring that time. Only in the rare occasion that I had to use Adobe Illustrator\nwould I boot into Windows. Finally with the WSL, using both Windows and Ubuntu\nis realistic.\nI’ve experimented with the WSL since early 2018 after David\nSmith provided instructions for installing R in\nthe\nWSL\nand Jeroen Ooms announced that you can host\nRStudio Server and OpenCPU apps from the\nWSL. However, the most\nI’ve used it for real work was for speed benchmarking my R package\nworkflowr.2 However, for one of my current freelance jobs, I\nwas provided a Windows laptop. With RStudio and Git\nBash, I’ve been productively developing on Windows\nfor the past couple of months. But it has been noticeably slower. Recently I had\nsome code that was taking minutes to run on Windows, so I tried it on the WSL,\nand it ran it in seconds. This finally convinced me that I should really switch\nover to primarily using the WSL.\nIn this post, I’ll describe how to install the WSL, configure it for R\ndevelopment, and share other tips and tricks.\nInstall Windows Subsytem for Linux from the Windows Store\nThe easiest method for installing the WSL is to use the Windows Store. Follow\nthese steps:\nConfirm that the WSL feature is enabled for Windows.\nOpen the Settings app, search for “windows features”, and select “Turn\nWindows features on or off”. Annoyingly I can’t find a way to navigate to\nthese settings without having to use the search bar.\n\nScroll down and confirm that Windows Subsystem for Linux is enabled. If\nit wasn’t previously enabled, you’ll need to restart your computer.\n\n\nOpen the Microsoft Store app (you open it like any other app), search for\n“Ubuntu”, and choose the most recent LTS version (currently 18.04).\n\nClick on “Install” (or “Get” if you access the Microsoft Store in the\nbrowser) and login with your Microsoft Account.\nOpen the Ubuntu app. Enter a username and password. As the directions state,\nthis username and password is in no way related to your Windows user\naccount.\n\nInstall Windows Subsytem for Linux from PowerShell\nOn my work laptop, IT blocked the Microsoft Store app. And when I tried the\nMicrosoft Store via the browser, the “Get” link failed (in multiple browsers, so\nI assume IT somehow also blocked this). If you find yourself in a similar\nsituation, or if you simply don’t want to create a Microsoft Account, you can\nperform all the steps above from a PowerShell terminal.\nEnable the WSL feature and restart your computer\n(source).\nEnable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux\nDownload Ubuntu\n(source).\nInvoke-WebRequest -Uri https://aka.ms/wsl-ubuntu-1804 -OutFile Ubuntu.appx -UseBasicParsing\nInstall Ubuntu\n(source).\nAdd-AppxPackage .\\Ubuntu.appx\nOpen Ubuntu and configure your username and password, as described above.\nInstall software with APT\nYou can install software using the package manager\nAPT. The very first thing you\nshould do after installing Ubuntu is to upgrade the software. You’ll need to\nenter your Ubuntu password to run commands as the root user (sudo).\n# Check for latest package versions\nsudo apt update\n# Upgrade currently installed packages\nsudo apt upgrade\n# Install emacs and unzip. Enter \"y\" to accept\nsudo apt install emacs unzip\nIt’s a good habit to regularly update the software by running sudo apt update && sudo apt upgrade.\nThe software versions available from the default sources used by APT eventually\nbecome outdated if you don’t regularly upgrade Ubuntu. Ubuntu 18.04 was\nreleased in April\n2018, so this is\ndefinitely a potential issue. The solution to this is to install more recent\nversions from Personal Package Archives\n(PPAs).\nFor example, the versions of Git available in the default channels are 2.17.0\nand 2.17.1.3\n$ apt list -a git\nListing... Done\ngit/bionic-updates,bionic-security,now 1:2.17.1-1ubuntu0.5 amd64 [installed,automatic]\ngit/bionic 1:2.17.0-1ubuntu1 amd64\nFortunately, the Git developers provide a PPA to install the latest stable\nupstream version, which can be added using\nthe command below:\nsudo add-apt-repository ppa:git-core/ppa\nAnd now Git 2.26.0 is available for download:\n$ apt list -a git\nListing... Done\ngit/bionic 1:2.26.0-1~ppa1~ubuntu18.04.1 amd64 [upgradable from: 1:2.17.1-1ubuntu0.5]\ngit/bionic-updates,bionic-security,now 1:2.17.1-1ubuntu0.5 amd64 [installed,upgradable to: 1:2.26.0-1~ppa1~ubuntu18.04.1]\ngit/bionic 1:2.17.0-1ubuntu1 amd64\nRunning sudo apt upgrade will replace Git 2.17.1 with 2.26.0.\nAnd since this is a new environment, don’t forget to set your user.name and\nuser.email to sign your Git commits:\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"email@domain\"\nInstall R\nR is available for installation with APT, but similar to Git, it is outdated (R\n3.4.4 was released in March 2018):\n$ apt list r-base\nListing... Done\nr-base/bionic 3.4.4-1ubuntu1 all\nThe official CRAN Ubuntu installation\ninstructions describe\nhow to manually edit the file /etc/apt/sources.list, which lists the sources\nfor APT to check for available software. However, since I am recommending the\nuse of PPAs, I think it is easier to directly add Michael Rutter’s RRutter v3.5\nPPA.4 This is the source PPA that is synced to CRAN,\nso it is identical to the official CRAN source.\nsudo add-apt-repository ppa:marutter/rrutter3.5\nInstead of manually editing /etc/apt/sources.list, the command above added the\nnew source in a separate file in the directory /etc/apt/sources.list.d/. Note\nthat the Git PPA is also there. For me, this makes it easier to manage PPAs. You\ncan quickly view which PPAs you are using, and you know which file to edit if\nyou need to update or disable the PPA.\n$ ls /etc/apt/sources.list.d/\ngit-core-ubuntu-ppa-bionic.list  git-core-ubuntu-ppa-bionic.list.save  marutter-ubuntu-rrutter3_5-bionic.list\nNow R 3.6.3 is available to install:\n$ apt list -a r-base\nListing... Done\nr-base/bionic 3.6.3-1bionic all\nr-base/bionic 3.4.4-1ubuntu1 all\nBefore you install R, an optional but advisable step is to register Michael\nRutter’s signing\nkey to\nensure the binaries you are installing on your machine are actually the ones he\nuploaded to his PPA. Note you’ll need to have admin access to your Windows\nmachine for this step.\nsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9\nIf you have trouble connecting to the server and you use a VPN, try\ndisconnecting from it. Also, the CRAN instructions have alternative commands to\ntry. The below option worked for me on my work laptop:\ngpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9\ngpg -a --export E298A3A825C0D65DFD57CBB651716619E084DAB9 | sudo apt-key add -\nInstall r-base (which includes r-base-core, r-base-dev, the documentation\nfiles, and the set of recommended packages in r-recommended).\nsudo apt install r-base\nInstall R packages\nTo install R packages, you could rely on install.packages(). But unlike\nWindows and macOS, CRAN does not build binaries for any of the Linux\ndistributions. This means that every package is installed from source, so\npackages like stringi will take minutes to install each time there is a new\nrelease. Thus I try as much as possible to rely on the binaries built for\nUbuntu.5\nThere are almost 500 R package binaries available for Ubuntu.\n$ apt list r-cran-* 2> /dev/null | wc -l\n496\nUnfortunately the RRutter v3.5 PPA (and its equivalent CRAN sync) added above\nonly includes R itself and the dozen or so recommended packages. The remaining\npackages will be the version from when R 3.4.4. was released years ago, which\nisn’t very useful.\nLuckily Michael Rutter maintains a second PPA,\ncran2deb4ubuntu_3.5,\nthat builds binaries for all the packages listed in the CRAN Task\nViews (and their package dependencies),\nand these packages are regularly updated. It’s not immediate, but if you\nregularly upgrade your APT packages, you’ll have the latest version of any given\nR package within a few days of its release to CRAN. Add this PPA to access over\n4000 R package binaries.\nsudo add-apt-repository ppa:marutter/c2d4u3.5\n$ apt list r-cran-* 2> /dev/null | wc -l\n4525\nThis allows you to install the entire tidyverse suite of R packages plus their\nsystem dependencies in only a few minutes.\nsudo apt install r-cran-tidyverse\nIf a package you want isn’t available when you try to install r-cran-foo (note\nthat the R package should always be lowercase), then you can install it with\ninstall.packages() into a personal library directory.\nNote that there is a downside to using the pre-built binary R packages. Since\nthey were already built, they don’t include any system dependencies that are\nonly required for compiling the package. For example, you can install and use\nthe RCurl package with sudo apt install r-cran-rcurl, but still get an error\nwhen running install.packages(\"RCurl\"). This is because RCurl requires the\nsystem library libcurl4 to run,\nbut it additionally needs the system library\nlibcurl4-openssl-dev\nto compile the package from source. Thus if you find yourself installing R\npackages from source because they aren’t available from a PPA, you’ll want to\ninstall some of the commonly required system libraries like\nlibcurl4-openssl-dev:\nsudo apt install libcurl4-openssl-dev\nFor other system software you may need to install, check out the Dockerfile for\nthe rhub Ubuntu\nimage.\nInstall Bioconductor packages\nThere are a decent number of Bioconductor packages available for Ubuntu, but\nthey are all outdated.\n$ apt list r-bioc-* 2> /dev/null | wc -l\n84\nThus I recommend installing\nBiocManager with APT. It’s\npart of the c2d4u PPA, so it will be kept up-to-date.\nsudo apt install r-cran-biocmanager\nAnd then you can install individual Bioconductor packages as needed from within\nR:\nBiocManager::install(c(\"GenomicRanges\", \"SummarizedExperiment\"))\nThis will unfortunately take awhile since the packages are built from source.\nIf some packages fail to install, it is likely due to the new staged install\ncheck\nintroduced in R 3.6. I got this error trying to install\nBiobase, and others have\nreported WSL-specific issues, e.g. for\nBiobase\nand\nps.\nA workaround is to disable this check prior to installation by setting an\nenvironment variable with Sys.setenv(R_INSTALL_STAGED = FALSE).\nInstall OpenCPU server\nUnless you’re a devoted emacs or vi user, you probably aren’t overly excited\nabout the prospect of doing all of your R development from a bash shell. You’re\nin luck! As I mentioned at the beginning of the post, Jeroen Ooms’ amazing\nOpenCPU project conveniently provides\nall the infrastructure to install and serve RStudio Server via the Apache web\nserver.6\nAs you might have guessed, the first step is to add the OpenCPU PPA. Then you\ncan install OpenCPU server, which also installs the Apache web server, and\nRStudio Server. Note that you will be prompted about email addresses. Since you\nare not setting up an email server, you can accept the default settings. Also,\nyou’ll again need to have admin rights to install Apache and RStudio Server.\nsudo add-apt-repository -y ppa:opencpu/opencpu-2.1\nsudo apt install opencpu-server\nsudo apt install rstudio-server\nNote that the OpenCPU\nPPA is versioned,\ni.e. the 2.1. Instead of updating the software in one PPA, he creates new ones\n(thus each PPA is stable long-term). Therefore you might want to check back in\nthe future to see if there is a new PPA available.7\nNow you should be able to navigate to http://localhost/rstudio/ with your\nbrowser. This is the RStudio Server you are running in the Ubuntu WSL. Login\nwith your Ubuntu username and password, and then run sessionInfo() to confirm.\nRStudio Server is setup automatically when you first install it. However, the\nnext time you start your machine and open Ubuntu, it won’t be ready to use. Thus\nI always create a file called start.sh that contains the startup steps from\nthe OpenCPU Server\nManual:\n#!/bin/bash\nset -eux\n\n# Enable OpenCPU for Apache\na2ensite opencpu\n\n# Restart Apache\napachectl restart\n\n# Start RStudio Server\nrstudio-server start\nYou’ll need root access, so execute it with sudo bash start.sh. Then you’ll\nonce again be able to access RStudio Server via your browser at\nhttp://localhost/rstudio/. Importantly, the RStudio Server will continue running\neven if you close the Ubuntu terminal. Thus the only time you need to re-run\nstart.sh is after you shutdown your computer.\nThe above instructions work fine on my personal laptop running Windows 10 Home\nedition. However, I wasn’t able to use port 80 on my work laptop running Windows\n10 Pro.\n$ sudo bash start.sh\n+ a2ensite opencpu\nSite opencpu already enabled\n+ apachectl restart\nhttpd not running, trying to start\n(13)Permission denied: AH00072: make_sock: could not bind to address [::]:80\n(13)Permission denied: AH00072: make_sock: could not bind to address 0.0.0.0:80\nno listening sockets available, shutting down\nAH00015: Unable to open logs\nAction 'restart' failed.\nThe Apache error log may have more information.\nIf you also receive the error about not being able to bind to port 80, then\nfollow the instructions in this Ask Ubuntu\nanswer.\nThe default port used by Apache is 80, which you can verify by viewing the ports\nconfiguration file.\n$ cat /etc/apache2/ports.conf\n# If you just change the port or add more ports here, you will likely also\n# have to change the VirtualHost statement in\n# /etc/apache2/sites-enabled/000-default.conf\n\nListen 80\n\n<IfModule ssl_module>\n        Listen 443\n<\/IfModule>\n\n<IfModule mod_gnutls.c>\n        Listen 443\n<\/IfModule>\n\n# vim: syntax=apache ts=4 sw=4 sts=4 sr noet\nTo get around a blocked port 80, edit that file (sudo nano /etc/apache2/ports.conf) so that Apache listens on any high port, e.g. Listen 8080. After changing the port, you can access RStudio Server in the browser by\nincluding the port in the URL, e.g. http://localhost:8080/rstudio/.\nNote that you’ll still probably receive a warning like [core:warn] [pid 62] (92)Protocol not available: AH00076: Failed to enable APR_TCP_DEFER_ACCEPT. You\ncan safely ignore this because its a known issue and it doesn’t affect Apache’s\nability to run (source).\nSince we are discussing editing ports, it’s a good time to discuss security.\nWith this setup, you are running a web server on your local machine. Now this\nshouldn’t be a big deal, since if your computer was open to outside network\ntraffic, you’d have way bigger problems\n(source).\nBut if you are worried about any potential issue, you can be overly cautious and\nrestrict Apache to only listen to ports on your local machine, i.e. localhost\nor the equivalent IP address 127.0.0.1. In other words, edit\n/etc/apache2/ports.conf, replacing Listen 8080 and Listen 443 with Listen 127.0.0.1:8080 and Listen 127.0.0.1:443\n(source).\nSSH keys\nTo make it easy to clone, push, and pull repositories from online Git hosting\nservices, e.g. GitHub, you’ll want to create SSH keys on the WSL and register\nthem with the service. I recommend the instructions from\nGitHub,\nwhich I’ve modified below. Note that I added the flag -o to use the newer,\nmore secure OpenSSH format for creating the key.\n# Create new SSH keys\nssh-keygen -o -t rsa -b 4096 -C \"Any message you would like to annotate the key\"\n# Start the SSH agent\neval \"$(ssh-agent -s)\"\n# Add SSH key to SSH agent\nssh-add ~/.ssh/id_rsa\nOnce you’ve created the key pair, you can copy the public key to\nGitHub\nand/or the other services you use.\nUnfortunately the experience with using SSH keys is not as seamless as it is\nwith a standard Ubuntu installation. If you use a passphrase with your SSH key,\nyou will be prompted to enter your passphrase every time. On standard Ubuntu,\nyou only have to enter it the first time you use your key after restarting your\ncomputer. I found a workaround for this, but it is hacky, so please let me know\nif you know of a better solution. First, add the line eval \"$(ssh-agent -s)\" >/dev/null to ~/.bashrc so that the agent is automatically started each time\nyou open a window. 8 Second, add the line\nAddKeysToAgent yes to the file ~/.ssh/config. Now you will only have to\nenter your passphrase once, though you will have to do it for every new window\nyou open.\nAccess Windows filesystem\nAnother nice feature of the WSL is that it is straightforward to access the\nfiles on your Windows machine. Unlike a virtual machine or Docker container, you\ndon’t have to take any extra steps to configure this. From Ubuntu, you can\naccess your Windows filesystem mounted at /mnt/c/. For example, a common use\ncase for me is to download a file from the internet to my Windows desktop, and\nthen copy it to the WSL for analysis.\ncp /mnt/c/Users/demo/Desktop/file.txt .\nHowever, transferring files between Ubuntu and Windows has its gotchas, so\nyou’ll want to use this feature sparingly. For example, if you want to access\nyour Git repos from Ubuntu and Windows, I’d recommend cloning them separately to\nWindows and Ubuntu, i.e. treating them as if they were on separate machines.\nSome guidelines:\nFile paths are case-sensitive on Ubuntu, but not on Windows. This can create\nsome real headaches. If you are accessing Windows files from Ubuntu via\n/mnt/c/, the WSL performs some magic so that the names are\nnon-case-sensitive. But you really don’t to rely on that behavior. Also, if\nyou are creating files from Ubuntu on Windows, you can configure the\ncase-sensitivity of the filenames on a per-directory\nbasis.\nBut again, I’d recommend you avoid this complication by naming all your files\non both Ubuntu and Windows such that they don’t rely on case to be\ndistinguished.\nWindows and Linux use different end of line characters to specify a\nnewline. This is the main reason I\nrecommended above that you should keep separate copies of Git repos, otherwise\nyou have configure your Git settings to handle different line\nendings,\notherwise it will constantly look like all of your files have been edited.\nConfusingly, it may appear that the line endings are fine when copying back\nand forth because they display ok. I recommend running dos2unix after\ncopying from Windows to Ubuntu, and unix2dos before copying from Ubuntu to\nWindows (you can install these tools with sudo apt install dos2unix).\nWhile you can, with caution, create or modify Windows files directly from\nUbuntu, never create or modify the Ubuntu files located in the AppData\ndirectory from\nWindows.\nIf you need to copy a file between the two, I recommend always doing this from\nthe Ubuntu terminal via /mnt/c/. If you really want to edit Ubuntu files\nfrom Windows, you can run explorer.exe . in the Ubuntu terminal to launch\nWindows File Explorer, or you can access them from PowerShell using the path\n\\\\wsl$\\{distro name}\\, e.g. ls \\\\wsl$\\Ubuntu-18.04\\home\\demo\\\n(source).\nVS Code remote extension\nAnother option for developing your code from the WSL is to use Visual Studio\nCode. You can install the extension for remote\ndevelopment in the WSL.9 This extension allows you to open VS Code in\nWindows, but switch to developing as if you were in the WSL. You can access all\nyour files, and if you open a terminal, it is a bash shell running in the WSL!\nSteps:\nOpen VS Code in Windows\nType Ctrl+Shift+P to open the command palette\nType wsl and then choose Remote-WSL: New Window (you’ll need admin rights\nto approve this the first time)\n\nOpen a file or folder in the WSL\nOpen a new terminal (Ctrl+Shift+`) to run bash commands\ndirectly in the WSL\n\nTo exit from the WSL, open the command palette and choose Remote: Close Remote Connection\n\nIf you have any issues with Git, there is documentation for fixing line ending\nissues\nand sharing credentials between Windows and the\nWSL\n(but you shouldn’t need to worry about the latter if you created SSH keys\nabove).\nAnd if you find you really like VS Code, you can configure it for R development.\nTo learn more, check out these blog posts from Kun\nRen (one of\nthe co-developers of the\nlanguageserver package),\nMiles McBain, and Jozef\nHajnala.\nRun Bash terminal from RStudio\nIf you only want to occasionally test your code on Ubuntu, you can open a\nterminal running bash in the WSL directly in RStudio.\nSteps:\nOpen RStudio in Windows\nGo to Tools->Global Options...->Terminal\nChange New terminals open with: to Bash (Windows Subsystem for Linux)\nOpen a new terminal with Alt+Shift+R\nRun pwd to confirm the terminal is running in the WSL\nType R to start the R console in the WSL terminal\nSend code from your R scripts directly to the terminal with\nCtrl+Alt+Enter10\nCopy and paste to and from terminal\nIf you are familiar with the behavior of the terminal window on Linux and macOS,\nyou will find the Windows terminal to be very strange. However, if you’ve used\nGit Bash before, then you will already know how it works.\nTo copy text from the terminal, highlight the text and press Enter.\nAlternatively, you can right-click the Ubuntu icon in the top left of the\nwindow, select Edit, and finally select Copy. Now you can paste into another\nwindow with Ctrl+V.\n\nTo paste text into the terminal, right click with the mouse. Alternatively, you\ncan use the menu: Edit->Paste. You can also try Ctrl+V or the Insert\nkey to see if either of those happen to work on your computer.\n\nWSL2\nTechnically everything I have said above applies to WSL1. The new\nWSL2 is a rewrite\nthat changes the technical implementation of running Linux on Windows (it uses a\nlightweight VM to run the Linux kernel), with the goal of faster file access.\nFrom the user perspective, you shouldn’t have to worry much if you want to try\nout the WSL2. You should still be able to access RStudio Server at\nhttp://localhost/rstudio/\n(source).\nAnd you can switch back and forth between WSL1 and WSL2, or run them both.\nHowever, it is still in development, so you may have to do some more\ntroubleshooting. For example, the VS Code Remote-WSL\nextension\nexplicitly states that its support for the WSL2 is experimental.\nIf you’re feeling ambitious, you can install the\nWSL2. Also, see this\npost for using the WSL 2 with Visual Studio\nCode. Note that you’ll\nneed a Windows account to be able to access this new feature via the Windows\nInsiders Program.\nConclusions\nAfter following these steps, hopefully you are now able to productively develop\nyour R code on Ubuntu running via the WSL. Please let me know if you ran into\nany issues or if a critical step is missing. Also, please let me know if you have\nany tips and tricks to share, especially if you have a better strategy for\nactivating the SSH agent.\nOther resources\nDave Tang’s post on Setting up Windows for bioinformatics in 2019\nCanonical’s instructions for installing Ubuntu with the WSL\nRStudio’s instructions for Using RStudio Server in Windows WSL2\n\nNot to\nmention that sharing host files with the VM requires additional\nsetup.↩\nFor whatever reason,\nread/write operations and filepath manipulations, e.g. converting an absolute\npath to a relative path, take a lot longer on Windows 10. On the WSL, which has\naccess to the same computational resources on my laptop as Windows, workflowr\nfunctions run a lot faster.↩\nNote that bionic refers to Bionic Beaver, the release name for\nUbuntu 18.04.↩\nDon’t be\nconcerned about the 3.5. There was a big change between versions 3.4 and 3.5 in\nhow R was packaged for Debian/Ubuntu. We’re far enough away from that event that\nyou shouldn’t have to worry about it. Just remember that for the latest version\nof R, you want the 3.5 version.↩\nSee this recent blog post from Jumping Rivers on various strategies for\nFaster R package\ninstallation.↩\nServing RStudio Server is a very limited use case for OpenCPU. It’s\nprimary purpose is for converting R code into web applications. This post is\nalready too long to include a detailed description, but you should check out the\nexample apps to get a sense of the\npossibilities.↩\nAlso note that his PPA\ncontains r-base and the r-recommended packages. These will eventually become\noutdated, but since you’ve added the RRutter v3.5\nPPA, you’ll\ncontinue to get the latest version of R from there. Thus you are only relying on\nthe OpenCPU PPA for OpenCPU server, Apache, and RStudio Server.↩\nI got this idea from this blog\npost.\nHowever, I don’t agree with all of his conclusions. He says that the ssh-agent\ndoesn’t persist. When I run ps -ef | grep ssh-agent, it clearly persists\nacross sessions. But for whatever reason it doesn’t work, and a new agent has to\nbe started. One reason I feel this is so hacky is that each time I open a\nwindow, a new ssh-agent process is spawned.↩\nThere\nare similar extensions for remote development in Docker\ncontainers or in any\nremote machine accessible via\nSSH, e.g. an HPC cluster. I’ve\nused all 3, and they are amazing.↩\nUnfortunately none of the other RStudio panes are aware\nof R running in the terminal. Plots won’t display, and the Environment pane\nwon’t update. If you need these features, use the RStudio Server strategy\ndescribed above.↩\n",
    "preview": "posts/wsl-r/img/windows-store-ubuntu.png",
    "last_modified": "2023-01-18T16:46:24+00:00",
    "input_file": {},
    "preview_width": 947,
    "preview_height": 888
  },
  {
    "path": "posts/pairwise-overlaps/",
    "title": "How to efficiently calculate pairwise overlaps of many sets",
    "description": "An example of progressively optimizing the speed of a function",
    "author": [
      {
        "name": "John Blischak",
        "url": "https://jdblischak.com"
      }
    ],
    "date": "2020-03-27",
    "categories": [
      "Software Development with R"
    ],
    "contents": "\nTable of Contents\nThe combinatorial explosion of pairwise overlaps\nSimulating sets\nFirst pass implementation\nCombinations and indexing\nPreallocation\nCheck sets once instead of every overlap\nConclusions\nFor one of my projects, I needed to calculate the pairwise overlaps of many sets, and my initial brute force approach was unbearably slow. And even after making some of the more obvious optimizations, it was still quite slow. Fortunately I got a good tip from Nir Graham when I posted on RStudio Community. Below I explain the problem in more detail, and then demonstrate how I progressively optimized the function for increased speed.1\nThe final solution is still brute force, just faster. If you know of a clever algorithm for finding pairwise overlaps2, please do get in touch.\nThe combinatorial explosion of pairwise overlaps\nFirst, why do I need to do this? I suspect there are many use cases for computing pairwise overlaps, but in my specific case I wanted to compare the similarity of biological annotation terms after having performed a gene set enrichment anlaysis. For those unfamiliar with “omics”-style analyses, the results are usually a list of biological features with varying levels of statistical signficance (e.g. which proteins change in relative abundance between control versus treated cells). To help interpretation, the list of results is compared to curated databases of annotation terms such as Gene Ontology categories or KEGG pathways. While they are usually more sophisticated, conceptually you can imagine a chi-squared test of a contingency table comparing the features in the results list with the features in each annotation category:\nTest\nFeature in results\nFeature not in results\nFeature in annotation\nx\ny\nFeature not in annotation\nz\nw\nIf a given annotation category contains more of the results than would be expected by chance, then this category is said to be “enriched”.\nA major hurdle for interpreting enrichment results is that the annotation categories often contain many of the same features, and thus the results can be very redundant. For example, how different are the categories cellular response to stress and response to stress? Thus it would be nice to have a metric of how related the categories are when interpeting the enrichment results.\nSpecifically I’d like to calculate three measurements:\nThe number of overlapping features\nThe overlap coefficient: \\(\\frac{| X \\cap Y |}{min(|X|, |Y|)}\\)\nThe Jaccard index: \\(\\frac{| X \\cap Y |}{| X \\cup Y |}\\)\nThis will be especially useful for a network visualization of the enrichment results, where the color of the nodes could correspond to the statistical significance, and the thickness of the eges could correspond to the Jaccard index. For example, the hypothetical results below demonstrate that there are two main sources of enrichment, even though five categories are statistically significant:\n\n\n\nThe essential problem that this post deals with is the fact that as the number of sets increases, the number of pairwise overlaps to be calculated explodes. 100 sets require 4,950 pairwise overlaps, but 1000 sets require 499,500. To be more precise, this is a combinatorial relationship, in which the number of pairwise overlaps is determined by the number of unique combinations of size 2 of the \\(n\\) sets:\n\\[ \\binom{n}{2} = \\frac {n!} {2!(n-2)!} \\] In R, I’ll use the function choose() to perform this calculation.\nBelow is a visualization of the combinatorial explosion in the number of pairwise overlaps:\n\n\n\nIn my application, I have multiple biological annotations that have thousands of sets to compare. Thus I need the overlap calculations to be quick since they’ll be performed millions of times.\nSimulating sets\nTo benchmark my implementations, I created the function simulate_sets() to create a list with a given number of elements containing character vectors.\n\n\nsimulate_sets <- function(n_sets) {\n  universe <- c(letters, LETTERS)\n  sets <- replicate(n_sets,\n                    sample(universe, size = rpois(1, lambda = 15)),\n                    simplify = FALSE)\n  names(sets) <- paste0(\"set\", seq(n_sets))\n  return(sets)\n}\n\nThe elements in the sets consist of lowercase and uppercase letters. Here’s what the output looks like:\n\n\nset.seed(12345)\n(sets_5 <- simulate_sets(n_sets = 5))\n\n$set1\n [1] \"p\" \"z\" \"B\" \"x\" \"C\" \"k\" \"F\" \"W\" \"b\" \"v\" \"U\" \"L\" \"M\" \"D\" \"j\" \"q\"\n[17] \"T\"\n\n$set2\n [1] \"L\" \"D\" \"a\" \"l\" \"t\" \"h\" \"W\" \"c\" \"i\" \"n\" \"m\" \"V\" \"p\" \"N\"\n\n$set3\n [1] \"H\" \"O\" \"F\" \"y\" \"J\" \"N\" \"k\" \"L\" \"i\" \"e\" \"o\" \"s\" \"q\" \"U\" \"K\" \"c\"\n[17] \"Y\" \"m\"\n\n$set4\n [1] \"W\" \"j\" \"Q\" \"A\" \"H\" \"R\" \"I\" \"S\" \"v\" \"P\" \"w\" \"E\" \"z\" \"g\" \"o\" \"K\"\n[17] \"X\" \"l\" \"N\" \"x\" \"c\"\n\n$set5\n [1] \"d\" \"k\" \"W\" \"c\" \"j\" \"o\" \"g\" \"P\" \"z\" \"Z\" \"B\" \"Q\" \"F\" \"r\" \"a\" \"S\"\n\nsets_100 <- simulate_sets(n_sets = 100)\nsets_1000 <- simulate_sets(n_sets = 1000)\n\nFirst pass implementation\nFor my first pass, I use a nested for loop to iterate over the names of the lists. The main advantages of this approach is that it was quick to write and it is straightforward to read the code. The essential part of this strategy is contained in the snippet below:\n\n\nfor (name1 in names(sets)) {\n  set1 <- sets[[name1]]\n  for (name2 in names(sets)) {\n    set2 <- sets[[name2]]\n  }\n}\n\nHere’s the entire function:\n\n\ncalc_pairwise_overlaps <- function(sets) {\n\n  vec_name1 <- character()\n  vec_name2 <- character()\n  vec_num_shared <- integer()\n  vec_overlap <- numeric()\n  vec_jaccard <- numeric()\n\n  for (name1 in names(sets)) {\n    set1 <- sets[[name1]]\n    for (name2 in names(sets)) {\n      set2 <- sets[[name2]]\n      set_intersection <- intersect(set1, set2)\n      num_shared <- length(set_intersection)\n      overlap <- num_shared / min(length(set1), length(set2))\n      jaccard <- num_shared / length(union(set1, set2))\n\n      vec_name1 <- c(vec_name1, name1)\n      vec_name2 <- c(vec_name2, name2)\n      vec_num_shared <- c(vec_num_shared, num_shared)\n      vec_overlap <- c(vec_overlap, overlap)\n      vec_jaccard <- c(vec_jaccard, jaccard)\n    }\n  }\n\n  result <- data.frame(name1 = vec_name1,\n                       name2 = vec_name2,\n                       num_shared = vec_num_shared,\n                       overlap = vec_overlap,\n                       jaccard = vec_jaccard,\n                       stringsAsFactors = FALSE)\n  return(result)\n}\n\nThere are of course lots of things to be improved, as you’ll see below, but if you have 100 sets or fewer, it probably wouldn’t be worth the effort to optimize it further.\n\n\nhead(calc_pairwise_overlaps(sets_100))\n\n  name1 name2 num_shared   overlap   jaccard\n1  set1  set1         21 1.0000000 1.0000000\n2  set1  set2          5 0.2777778 0.1470588\n3  set1  set3          8 0.4210526 0.2500000\n4  set1  set4          9 0.4500000 0.2812500\n5  set1  set5          7 0.6363636 0.2800000\n6  set1  set6          6 0.4000000 0.2000000\n\nsystem.time(calc_pairwise_overlaps(sets_100))\n\n   user  system elapsed \n  0.965   0.016   0.981 \n\nCombinations and indexing\nOne big issue with the strategy above is that each pairwise combination is performed twice. In other words, I calculated the overlap statistics for each permutation of the sets, when I really only need to calculate each combination. The second issue I’ll address in this iteration is how to fetch each set. Above I obtained each set by name, sets[[name1]], which requires searching over the names of the list to find a match. While convenient, this gets slower as the list grows. Thus instead of looping over the names of the list, it is better to use an index variable to track the position in the list. This allows the sets to be quickly obtained via their position in the list.\nThe essential part of this update is in the snippet below. The index variables i and j are used to obtain the name of the sets and their contents in each iteration. Furthermore, j is always greater than i, to avoid comparing a set to itself or repeating any previous combinations.\n\n\nn_sets <- length(sets)\nset_names <- names(sets)\nfor (i in seq_len(n_sets - 1)) {\n  name1 <- set_names[i]\n  set1 <- sets[[i]]\n  for (j in seq(i + 1, n_sets)) {\n    name2 <- set_names[j]\n    set2 <- sets[[j]]\n  }\n}\n\nHere’s the updated function:\n\n\ncalc_pairwise_overlaps <- function(sets) {\n\n  n_sets <- length(sets)\n  set_names <- names(sets)\n\n  vec_name1 <- character()\n  vec_name2 <- character()\n  vec_num_shared <- integer()\n  vec_overlap <- numeric()\n  vec_jaccard <- numeric()\n\n  for (i in seq_len(n_sets - 1)) {\n    name1 <- set_names[i]\n    set1 <- sets[[i]]\n    for (j in seq(i + 1, n_sets)) {\n      name2 <- set_names[j]\n      set2 <- sets[[j]]\n      set_intersection <- intersect(set1, set2)\n      num_shared <- length(set_intersection)\n      overlap <- num_shared / min(length(set1), length(set2))\n      jaccard <- num_shared / length(union(set1, set2))\n\n      vec_name1 <- c(vec_name1, name1)\n      vec_name2 <- c(vec_name2, name2)\n      vec_num_shared <- c(vec_num_shared, num_shared)\n      vec_overlap <- c(vec_overlap, overlap)\n      vec_jaccard <- c(vec_jaccard, jaccard)\n    }\n  }\n\n  result <- data.frame(name1 = vec_name1,\n                       name2 = vec_name2,\n                       num_shared = vec_num_shared,\n                       overlap = vec_overlap,\n                       jaccard = vec_jaccard,\n                       stringsAsFactors = FALSE)\n  return(result)\n}\n\nWith these two improvements, the computation time more than halved for a list of 100 sets. However, it is still quite slow when running 1000 sets.\n\n\nsystem.time(calc_pairwise_overlaps(sets_100))\n\n   user  system elapsed \n  0.300   0.016   0.316 \n\nAnd these improvements are essentially the advice from this StackOverflow answer on the same topic.\nPreallocation\nThe next issue I’ll address is a common cause of slow R code: growing objects during each iteration of a loop, e.g. vec_overlap <- c(vec_overlap, overlap). This may be convenient, but it is extremely slow. The solution is to preallocate the vectors to their final length, and then add new data by index.\nIn this case, the final length will be the total number of combinations, which I calculate using the function choose(). Furthermore, I add a new index variable, overlaps_index, to keep track of the combinations. The essential code to preallocate the vectors is in the code snippet below:\n\n\n# preallocate the vectors\nn_overlaps <- choose(n = n_sets, k = 2)\nvec_name1 <- character(length = n_overlaps)\nvec_name2 <- character(length = n_overlaps)\nvec_num_shared <- integer(length = n_overlaps)\nvec_overlap <- numeric(length = n_overlaps)\nvec_jaccard <- numeric(length = n_overlaps)\noverlaps_index <- 1\n\n# During each iteration, use the index to assign the latest value to the output\n# vectors\nvec_name1[overlaps_index] <- name1\nvec_name2[overlaps_index] <- name2\nvec_num_shared[overlaps_index] <- num_shared\nvec_overlap[overlaps_index] <- overlap\nvec_jaccard[overlaps_index] <- jaccard\noverlaps_index <- overlaps_index + 1\n\nAnd here’s the updated function:\n\n\ncalc_pairwise_overlaps <- function(sets) {\n\n  n_sets <- length(sets)\n  set_names <- names(sets)\n  n_overlaps <- choose(n = n_sets, k = 2)\n\n  vec_name1 <- character(length = n_overlaps)\n  vec_name2 <- character(length = n_overlaps)\n  vec_num_shared <- integer(length = n_overlaps)\n  vec_overlap <- numeric(length = n_overlaps)\n  vec_jaccard <- numeric(length = n_overlaps)\n  overlaps_index <- 1\n\n  for (i in seq_len(n_sets - 1)) {\n    name1 <- set_names[i]\n    set1 <- sets[[i]]\n    for (j in seq(i + 1, n_sets)) {\n      name2 <- set_names[j]\n      set2 <- sets[[j]]\n      set_intersection <- intersect(set1, set2)\n      num_shared <- length(set_intersection)\n      overlap <- num_shared / min(length(set1), length(set2))\n      jaccard <- num_shared / length(union(set1, set2))\n\n      vec_name1[overlaps_index] <- name1\n      vec_name2[overlaps_index] <- name2\n      vec_num_shared[overlaps_index] <- num_shared\n      vec_overlap[overlaps_index] <- overlap\n      vec_jaccard[overlaps_index] <- jaccard\n\n      overlaps_index <- overlaps_index + 1\n    }\n  }\n\n  result <- data.frame(name1 = vec_name1,\n                       name2 = vec_name2,\n                       num_shared = vec_num_shared,\n                       overlap = vec_overlap,\n                       jaccard = vec_jaccard,\n                       stringsAsFactors = FALSE)\n  return(result)\n}\n\nWith the preallocation, the overlaps of 100 sets is almost instantaneous:\n\n\nsystem.time(calc_pairwise_overlaps(sets_100))\n\n   user  system elapsed \n   0.09    0.00    0.09 \n\nAnd it can even handle 1000 sets in a reasonable amount of time:\n\n\nsystem.time(calc_pairwise_overlaps(simulate_sets(n_sets = 1000)))\n\n   user  system elapsed \n  7.912   0.020   7.932 \n\nBut I had thousands of sets to compare, and this was still slow.\nCheck sets once instead of every overlap\nAfter making the above changes, I was stumped as to how to further increase its speed, so I posted on RStudio Community for ideas. And I was lucky to get the advice from Nir Graham to speed up R’s set functions like intersect() by checking the inputs once instead of during every overlap calculation.\nThe function intersect() converts its inputs to vectors with as.vector() and also removes duplicates by calling unique().\n\n\nintersect\n\nfunction (x, y) \n{\n    y <- as.vector(y)\n    unique(y[match(as.vector(x), y, 0L)])\n}\n<bytecode: 0x55e29093f8a8>\n<environment: namespace:base>\n\nSince these calculations are performed during each pairwise overlap, they are extremely redundant. It is faster to confirm all the input sets are unique vectors at the very beginning of the function, since then the code no longer needs to check during each iteration of the loop. When valid input can be assumed, intersect() reduces to:\n\n\ny[match(x, y, 0L)]\n\nIt’s a similar situation with union(). It also converts its inputs to vectors with as.vector(). However, it does require the call to unique() to properly find the union of the two sets.\n\n\nunion\n\nfunction (x, y) \nunique(c(as.vector(x), as.vector(y)))\n<bytecode: 0x55e29117c3d8>\n<environment: namespace:base>\n\nBut even unique() itself can be sped up. Since there is no specific unique() method for character vectors (i.e. unique.character()), character vectors are processed with unique.default().\n\n\nmethods(unique)\n\n [1] unique.array           unique.bibentry*      \n [3] unique.data.frame      unique.default        \n [5] unique.igraph.es*      unique.igraph.vs*     \n [7] unique.matrix          unique.numeric_version\n [9] unique.POSIXlt         unique.vctrs_sclr*    \n[11] unique.vctrs_vctr*     unique.warnings       \nsee '?methods' for accessing help and source code\n\nunique.default\n\nfunction (x, incomparables = FALSE, fromLast = FALSE, nmax = NA, \n    ...) \n{\n    if (is.factor(x)) {\n        z <- .Internal(unique(x, incomparables, fromLast, min(length(x), \n            nlevels(x) + 1L)))\n        return(factor(z, levels = seq_len(nlevels(x)), labels = levels(x), \n            ordered = is.ordered(x)))\n    }\n    z <- .Internal(unique(x, incomparables, fromLast, nmax))\n    if (inherits(x, \"POSIXct\")) \n        structure(z, class = class(x), tzone = attr(x, \"tzone\"))\n    else if (inherits(x, \"Date\")) \n        structure(z, class = class(x))\n    else z\n}\n<bytecode: 0x55e28eee5e58>\n<environment: namespace:base>\n\nAnd the majority of unique.default() is code to handle various types of input like factors, times, or dates. Since I will ensure the input will always be character vectors, the only required line of unique.default() is the one below:\n\n\nz <- .Internal(unique(x, incomparables, fromLast, nmax))\n\nSo in this final optimized function, the input sets are first confirmed to be unique character vectors at the very beginning of the function, and then this assumption is never tested unnecessarily again.\n\n\ncalc_pairwise_overlaps <- function(sets) {\n  # Ensure that all sets are unique character vectors\n  sets_are_vectors <- vapply(sets, is.vector, logical(1))\n  if (any(!sets_are_vectors)) {\n    stop(\"Sets must be vectors\")\n  }\n  sets_are_atomic <- vapply(sets, is.atomic, logical(1))\n  if (any(!sets_are_atomic)) {\n    stop(\"Sets must be atomic vectors, i.e. not lists\")\n  }\n  sets <- lapply(sets, as.character)\n  is_unique <- function(x) length(unique(x)) == length(x)\n  sets_are_unique <- vapply(sets, is_unique, logical(1))\n  if (any(!sets_are_unique)) {\n    stop(\"Sets must be unique, i.e. no duplicated elements\")\n  }\n\n  n_sets <- length(sets)\n  set_names <- names(sets)\n  n_overlaps <- choose(n = n_sets, k = 2)\n\n  vec_name1 <- character(length = n_overlaps)\n  vec_name2 <- character(length = n_overlaps)\n  vec_num_shared <- integer(length = n_overlaps)\n  vec_overlap <- numeric(length = n_overlaps)\n  vec_jaccard <- numeric(length = n_overlaps)\n  overlaps_index <- 1\n\n  for (i in seq_len(n_sets - 1)) {\n    name1 <- set_names[i]\n    set1 <- sets[[i]]\n    for (j in seq(i + 1, n_sets)) {\n      name2 <- set_names[j]\n      set2 <- sets[[j]]\n\n      set_intersect <- set1[match(set2, set1, 0L)]\n      set_union <- .Internal(unique(c(set1, set2), incomparables = FALSE,\n                                    fromLast = FALSE, nmax = NA))\n      num_shared <- length(set_intersect)\n      overlap <- num_shared / min(length(set1), length(set2))\n      jaccard <- num_shared / length(set_union)\n\n      vec_name1[overlaps_index] <- name1\n      vec_name2[overlaps_index] <- name2\n      vec_num_shared[overlaps_index] <- num_shared\n      vec_overlap[overlaps_index] <- overlap\n      vec_jaccard[overlaps_index] <- jaccard\n\n      overlaps_index <- overlaps_index + 1\n    }\n  }\n\n  result <- data.frame(name1 = vec_name1,\n                       name2 = vec_name2,\n                       num_shared = vec_num_shared,\n                       overlap = vec_overlap,\n                       jaccard = vec_jaccard,\n                       stringsAsFactors = FALSE)\n  return(result)\n}\n\nAgain, 100 sets is calculated almost instantaneously.\n\n\nsystem.time(calc_pairwise_overlaps(sets_100))\n\n   user  system elapsed \n  0.057   0.000   0.057 \n\nAnd even 1000 sets is finished processing in only a few seconds:\n\n\nsystem.time(calc_pairwise_overlaps(sets_1000))\n\n   user  system elapsed \n  2.444   0.011   2.456 \n\nConclusions\nIn this post, I demonstrated how to iteratively increase the speed of a slow function. The biggest lesson I learned from this experience is that it is possible to increase the speed of base R functions since they may be designed to handle multiple different types of inputs.\nIf you see any ways that this function could be further optimized, please do let me know!\nClick here to view the R session information: \n\n\nsessionInfo()\n\nR version 3.6.3 (2020-02-29)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 18.04.4 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.7.1\nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.7.1\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n[1] tidygraph_1.1.2 ggraph_2.0.2    ggplot2_3.3.0   rstudioapi_0.11\n[5] rmarkdown_2.1   fs_1.4.0        git2r_0.26.1   \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.4         compiler_3.6.3     pillar_1.4.3      \n [4] viridis_0.5.1      tools_3.6.3        digest_0.6.25     \n [7] viridisLite_0.3.0  evaluate_0.14      lifecycle_0.2.0   \n[10] tibble_3.0.0       gtable_0.3.0       pkgconfig_2.0.3   \n[13] rlang_0.4.5        igraph_1.2.5       cli_2.0.2         \n[16] ggrepel_0.8.2      distill_0.7        yaml_2.2.1        \n[19] xfun_0.11          gridExtra_2.3      withr_2.1.2       \n[22] stringr_1.4.0      dplyr_0.8.5        knitr_1.28        \n[25] graphlayouts_0.6.0 vctrs_0.2.4        grid_3.6.3        \n[28] tidyselect_1.0.0   glue_1.3.2         R6_2.4.1          \n[31] fansi_0.4.1        polyclip_1.10-0    farver_2.0.1      \n[34] tweenr_1.0.1       tidyr_1.0.2        purrr_0.3.3       \n[37] magrittr_1.5       MASS_7.3-51.5      scales_1.1.0      \n[40] htmltools_0.4.0    ellipsis_0.3.0     assertthat_0.2.1  \n[43] ggforce_0.3.1      colorspace_1.4-1   labeling_0.3      \n[46] stringi_1.4.6      munsell_0.5.0      crayon_1.3.4      \n\nNote that I didn’t take these exact steps. Real life is a lot messier. I purposefully chose to break the process down into the steps below for didactic purposes.↩\nFor example, this SO answer suggests a pseudo-mergesort might be a potential solution, but doesn’t provide an implementation.↩\n",
    "preview": "posts/pairwise-overlaps/pairwise-overlaps_files/figure-html5/combinatorial-explosion-1.png",
    "last_modified": "2023-01-18T16:46:24+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/rstudioconf2020/",
    "title": "Some themes from rstudio::conf 2020",
    "description": "There were a lot of great talks at rstudio::conf. In this post I highlight a\nfew of the themes that emerged for me.",
    "author": [
      {
        "name": "John Blischak",
        "url": "https://jdblischak.com"
      }
    ],
    "date": "2020-02-12",
    "categories": [
      "conference"
    ],
    "contents": "\nTable of Contents\nPut R in production\nR Markdown continues to be awesome\nParallel processing\nProgramming with the tidyverse\nMiscellaneous\nI had a great time last month at rstudio::conf 2020 in San Francisco. I got to catch up with my friends in the R community, make new ones, and learn about some of the latest developments for R and RStudio. Below I highlight some of the main themes from my conference experience and point to some representative talks for each one.\nDisclaimer: I’ve only included talks that I saw in person. Due to space and time constraints, I wasn’t able to attend every talk I wanted to (there were 4 parallel tracks!). Also, I’ve tried to focus on talks that can be grouped into larger themes. In other words, below is a very small subset of the many great talks! For more resources, see the videos hosted by RStudio as well as the GitHub repository RStudioConf2020Slides by Emil Hvitfeldt.\nMy badge and hex stickers from rstudio::conf 2020.Put R in production\nI think one of the most exciting developments in the R community is the increasing focus on putting R in production. Instead of re-writing your R code in a different language or passing off your code to someone else to deploy, there continue to be more resources available for R users to directly deploy their applications.\nAlex Gold - Deploying End-To-End Data Science with Shiny, Plumber, and Pins\nVideo\nGitHub\nBlog post\n\n\n\nNice example from @alexkgold of deploying a machine learning model using #rstats packageshttps://t.co/AAaEjcuwrs#rstudioconf2020\n\n— John Blischak (@jdblischak) January 29, 2020\n\nHeather and Jacueline Nolis - We’re hitting R a million times a day so we made a talk about it\nVideo\nSlides (PDF)\nPut R in prod\n\n\n\nSage advice for putting R models in production from @heatherklus and @skyetetra:- Avoid too many tests by only testing the most critical behavior- Load test to find bottlenecks- Give people a tool to explore and understand the model#rstudioconf2020\n\n— John Blischak (@jdblischak) January 29, 2020\n\n\n\nAlso check out their great documentation on putting R in production at https://t.co/AJsO3MkRvk\n\n— John Blischak (@jdblischak) January 29, 2020\n\nR Markdown continues to be awesome\nI’m obviously biased since I love R Markdown so much that I created an entire project framework on top of it (workflowr), but trust me that it really is awesome!\nYihui Xie - One R Markdown Document, Fourteen Demos\nYihui nicely demonstrated the versatility of the R Markdown format and its many potential outputs, including the newer rolldown package\nVideo\nBlog post\n\nEmily Riederer - RMarkdown Driven Development (RDD)\nVideo\nOriginal blog post on RDD\nLatest blog post on RDD\n\n\n\nGreat advice from @EmilyRiederer on using R Markdown to structure and progressively refine an analysis. If you missed her talk, check out her blog post:#rstudioconf2020https://t.co/83xCmic1RS https://t.co/i2VY94G28E\n\n— John Blischak (@jdblischak) January 30, 2020\n\nParallel processing\nI don’t take advantage of parallel processing nearly as often as I probably should, so it was nice to get great overview of the latest developments to make parallel code both easier to write and also more robust.\nBryan Lewis - Parallel computing with R using foreach, future, and other packages\nI highly recommend this dynamic talk by Bryan. In a single talk he was able to explain highly technical topics, keep the audience laughing, and mourn the loss of a friend.\nVideo\nGitHub\n\n\n\nFrom Bryan Lewis at #rstudioconf2020: A deep dive into the foreach package to parallelize your #rstats codehttps://t.co/Q8WBvGwbRp\n\n— John Blischak (@jdblischak) January 30, 2020\n\nHenrik Bengtsson - Future: Simple Async, Paralell & Distributed Processing in R - What’s Next?\nVideo\nSlides\nBlog post\n\n\n\nA new versatile #rstats package progressr from @henrikbengtsson for reporting progress updates #rstudioconf2020https://t.co/XTlywkb2tu\n\n— John Blischak (@jdblischak) January 30, 2020\n\nProgramming with the tidyverse\nThe tidyverse packages make routine data analysis procedures much more convenient; however, I know I’m not the only one that struggles when I attempt to use non-standard evaluation inside a function. Fortunately there were multiple talks on strategies for programming with the tidyverse.\nDewey Dunnington - Best practices for programming with ggplot2\nVideo\nVignette (dev) - Using ggplot2 in packages\nGitHub\n\n\n\nIf you're using ggplot2 in your package(s), check out this guide from @paleolimbot:https://t.co/XFAYq96xIX#rstudioconf2020 https://t.co/nvn0Olilta\n\n— John Blischak (@jdblischak) January 30, 2020\n\nLionel Henry - Advances in tidyeval (Interactivity and Programming in the Tidyverse)\nVideo\nSlides\nBlog post\n\n\n\nInteractivity and programming in the tidyverse. The slides of my #rstudioconf talk are available at https://t.co/vV1HXcj0vA pic.twitter.com/j8SeJzgKXB\n\n— lionel (@_lionelhenry) January 30, 2020\n\nMiscellaneous\nLastly, here are a few more talks I saw that I recommend.\nJenny Bryan - Object of type ‘closure’ is not subsettable\nJenny’s talks are always high-quality, and learning R’s debugging tools will help keep you sane when you have to hunt down a difficult bug\nVideo\nGitHub\n\n\n\n“object of type ‘closure’ is not subsettable”👆 is a talk I gave at #rstudioconf on getting unstuck and debugging in #rstatsSlides and other resources are here: https://t.co/9sOPBHUxRa pic.twitter.com/jzwSNpy1P2\n\n— Jenny Bryan (@JennyBryan) January 30, 2020\n\nColin Gillespie - How to win an AI Hackathon, without using AI\nColin gave an entertaining talk with an important message: understanding the problem to be solved and the data available can be more valuable than a fancy statistical model\nVideo\n\nMaria Ortiz Mancera - Mexican electoral quick count night with R\nMaria presented her team’s work to estimate the Mexican election results from an initial subsample (in order to prevent fraud and premature claims of victory). Her lightning talk stood out to me for two reasons. First, their work is directly supporting the integrity of democratic elections for an entire country. How many of us can say that our work has such an immediate and important impact on such a large number of people? They are literally defending democracy! Second, to avoid any outside interference, the analysts are isolated not only from other people but also the internet. Thus they have to perform this important work analyzing live data (which no doubt has to be cleaned!) without the ability to check online resources like StackOverflow. Very impressive!\nVideo\nGitHub\n\n\n\nReally interesting talk from @TeresaOM on predicting Mexican election results from initial polling data. They're in a “bunker” with no internet access, so no StackOverflow!#rstudioconf2020https://t.co/8Im6k6zusu\n\n— John Blischak (@jdblischak) January 30, 2020\n\n",
    "preview": "posts/rstudioconf2020/img/rstudioconf2020.jpg",
    "last_modified": "2023-01-18T16:46:24+00:00",
    "input_file": {}
  },
  {
    "path": "posts/circleci-ssh/",
    "title": "Automate commits to GitHub from a CircleCI job",
    "description": "Automatically update your documentation or other repository content by\nauthenticating CircleCI to push commits directly to GitHub",
    "author": [
      {
        "name": "John Blischak",
        "url": "https://jdblischak.com"
      }
    ],
    "date": "2020-02-04",
    "categories": [
      "devops"
    ],
    "contents": "\n\nContents\nInitial setup\nOverview and terminology\nGenerate one-off SSH keys\nAdd the public key to GitHub\nAdd the private key to CircleCI\nAdd the fingerprint to configuration file\nPush to GitHub\nDelete the local SSH key files\nConclusion\nAlternative authentication options\nLinks to further documentation\n\nContinuous integration (CI) is traditionally used for testing software and also\noptionally deploying the code to be executed on a different server. However, I\nsometimes want to be able to have the CI job commit changes back to the Git\nrepository that initially triggered the CI build. For example, if I have\nautomated a data analysis, I want the latest version of the report to be\ncommitted back the repository. Another example is committing the latest version\nof the documentation (e.g. to the gh-pages branch of a GitHub repository).\nAs I mentioned in my first post on using CircleCI, I have\nfound that configuring CircleCI to commit back to a Git repository to be easier\ncompared to other CI platforms. Below I detail the steps required.\nInitial setup\nThis post assumes you already have already done the following:\nCreated a GitHub repository\nActivated CI builds for the repository on CircleCI\nIf you haven’t done this, see the CircleCI documentation Getting\nStarted.\nOverview and terminology\nThe overall goal is to automatically save some artifact generated by a CI job\n(e.g. report, plot, documentation).1 To accomplish this, the main challenge is authenticating the CI\nserver to be able to commit to your repository on your behalf.\nIn this post, I will detail how to setup an SSH deploy key with write access to\nyour repository. I recommend this method because it is straight-forward and\nsecure. If your private key gets exposed (e.g. accidentially printing it in a CI\nlog), an attacker would only gain access to that one repository. In a later\nsection, I briefly explain alternative authentication\noptions.\nYou already have one deploy key setup for your repository. When you activate\nCircleCI to build a GitHub repository, it automatically adds a key to the GitHub\nrepository. In fact, you probably received an email from GitHub with the title\n“A new public key was added to <account>/<repository>” alerting you\nto the fact that this happened. You can navigate to view the key on GitHub by\ngoing to Settings -> Deploy Keys. Note that this key is named “CircleCI”.\nImportantly, it is “Read-only”. This means that CircleCI currently only has\npermission to read from the repository. If you add a git push to your CI job,\nit will fail with an authentication error.\nThus you need to generate a new pair of SSH keys. You will add the public key to\nyour GitHub repository as a deploy key with write access. Next you will\nadd the private key to the corresponding project on CircleCI. Lastly, you\nwill update the CircleCI configuration file so that it pushes to GitHub. Each\nof these steps is explained below.\nGenerate one-off SSH keys\nFirst you need to create some one-off SSH keys that will only be used by\nCircleCI to authenticate with this one GitHub repository. You can follow the\nstandard instructions from GitHub for generating a new SSH\nkey. However, there is one main difference. You will\nimmediately delete the key from your local computer afterwards, thus you want to\nmake sure you don’t break your current setup. If you already have SSH keys on\nyour machine, they are likely in the default location: ~/.ssh/. 2 You can use the flag -f to specify the location of\nthe output key files. Run the commands below to generate new SSH keys.3 When asked for a passphrase, press enter twice to not add\na passphrase to the key.\nmkdir /tmp/ssh-temp/\nssh-keygen -t rsa -b 4096 -C \"CircleCI Deploy Key with Write Access\" -f /tmp/ssh-temp/key\nThis created the private key /tmp/ssh-temp/key and the public key\n/tmp/ssh-temp/key.pub.\nIn order for the key to work, it must be in PEM format. Confirm this by\nrunning the code below. The first line of the private key field must be\n-----BEGIN RSA PRIVATE KEY-----.\nhead -n 1 /tmp/ssh-temp/key\nIf your version of ssh-keygen didn’t produce that line verbatim, delete the\nkeys and re-run the command with the additional flag -m PEM.\nAdd the public key to GitHub\nNext navigate to your repository on GitHub. Click on Settings and then select\n“Deploy keys” from the menu on the left. You should see that there is already a\n“Read-only” key named “CircleCI”. Click “Add deploy key”. You can name it\nwhatever you like.4 Then copy-paste the contents of the public key\n/tmp/ssh-temp/key.pub into the Key field. Lastly, check the box “Allow write\naccess” so that CircleCI can push back to GitHub.\nDeploy keys for GitHub repository. The top one is the default Read-only deploy key added automatically by CircleCI. The bottom one is the new Read/write deploy key.Add the private key to CircleCI\nNext navigate to the project on CircleCI. The URL follows the pattern:\nhttps://circleci.com/gh/<account>/<repo>.5\nDon’t click on the “Settings” in the menu on the left. That is for user\nsettings. Instead, click on the gear icon in the top right. Then choose “SSH\nPermissions” and “Add SSH key”. For the Hostname put “github.com” 6. Copy-paste the private key in /tmp/ssh-temp/key into the field\n“Private Key”. As the name implies, it is really important that you do not\nexpose this key.\nAdd private key to CircleCIAdd the fingerprint to configuration file\nAfter adding the private key to CircleCI, you will see it displays a fingerprint\nfor the key. You will need to add this key to the configuration file\n.circleci/config.yml in your repository using the step add_ssh_keys.\nCopy-paste the fingerprint into an add_ssh_keys step as shown below:\n    steps:\n      - checkout\n      - add_ssh_keys:\n          fingerprints:\n            - \"<copy-paste-fingerprint-here>\"\n      - run:\nPush to GitHub\nNow that CircleCI has permission to push to GitHub, you’ll need to add a step to\ndo this. It can look something like below. It configures Git, checks out the\nmaster branch,7 commits any\nchanges to all tracked files, and then pushes to GitHub. The --allow-empty\nprevents an error in the case where no files were changed. The [skip ci]\nprevents the job from running in an infinite loop due to constantly\nre-triggering the CI build.\n      - run:\n          name: Commit to GitHub\n          command: |\n            git config user.email \"<insert-the-email-you-use-with-github>\"\n            git config user.name \"CircleCI Job\"\n            git checkout master\n            git commit --allow-empty -am \"Automatic commit from CircleCI [skip ci]\"\n            git push origin master\nNow you probably don’t want to always attempt to push to GitHub. For example,\nwhen CI builds are triggered by a Pull Request. The updated version below\nrequires a specific CircleCI user account and that it is not a Pull Request.\nOtherwise it doesn’t attempt to commit.\n      - run:\n          name: Commit to GitHub\n          command: |\n            if [[ \"${CIRCLE_USERNAME}\" = \"<account>\" && -z \"${CIRCLE_PULL_REQUEST}\"]]\n            then\n              echo \"Committing to GitHub\"\n              git config user.email \"<insert-the-email-you-use-with-github>\"\n              git config user.name \"CircleCI Job\"\n              git checkout master\n              git commit --allow-empty -am \"Automatic commit from CircleCI [skip ci]\"\n              git push origin master\n            else\n              echo \"Not committing to GitHub\"\n            fi\nCommit this update to the local configuration file and then push to GitHub. This\ntime the build should result in a new commit to the repository.\nIf you receive an error due to a message such as the one below:\nThe authenticity of host 'github.com (192.30.253.113)' can't be established.\nRSA key fingerprint is SHA256:<fingerprint>.\nAre you sure you want to continue connecting (yes/no)?\nYou can bypass it by manually adding GitHub to the list of known SSH\nhosts prior to running git push:8\nssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts\nDelete the local SSH key files\nOnce you’ve confirmed everything is working as expected, delete the one-off SSH\nkeys from your local machine:\nrm -r /tmp/ssh-temp/\nConclusion\nNow any time you want CircleCI to automatically update the results of an\nanalysis or documentation files for a GitHub repository, you can follow these\nsteps to setup the authentication.\nLastly, I’ll note that a powerful combination is to schedule the\nworkflow to be run on CircleCI at regular intervals. Then\nthe repository will stay up-to-date without requiring any intervention.\nAlternative authentication options\nThere are alternative options for authentication:\nGitHub Personal Access Token (PAT): You could generate a GitHub PAT,\neither for your own account or a bot account, with the required scope to write\nto your repository (public_repo for a public repository, repo for a private\nrepository). You would then define this as an environment variable in your\nCircleCI build (e.g. GITHUB_PAT), which would authenticate you to push to your\nrepository using the following URL pattern:\nhttps://<account>:$GITHUB_PAT@github.com/<account>/<repo>.git. I dislike this\noption because it is not possible to limit the scope of a PAT to a single\nrepository. In other words, if a PAT with write permissions is exposed, it gives\nwrite access to every repository owned by that account. Update (2022-10-03):\nEven though GitHub has disabled password-based HTTP\nauthentication, I confirmed that it is still possible\nto perform PAT-based HTTP authentication from a CI build.\nSSH user key: CircleCI has a convenient option to register a user key.\nIn the CircleCI project settings page, choose “Checkout SSH keys” and then\n“Authorize with GitHub”. This will add a public key to your GitHub account,\nand CircleCI keeps the private key. Now your CI build will be able to access\nall of your public and private repositories on GitHub. I recommend this option\nif your CI build needs write access to multiple repositories (or read access to\nmultiple private repositories). If you only need write access to the one\nrepository, it is more secure to use a deploy key since it has limited access\nto your account.\nIf you want to create a user key, you can automate this even more using the\nR package ropenscilabs/circle developed by Patrick\nSchratz. It requires some initial setup: 1) generate a CircleCI API key\nand save it in ~/.Renviron as R_CIRCLE, and 2) generate a GitHub PAT\nand save it in ~/.Renviron as GITHUB_PAT. Then the user key can be\ncreated with the function use_circle_deploy().\nLinks to further documentation\nGitHub\nGenerating a new SSH key\n\nCircleCI\nAdding an SSH Key to CircleCI\nGitHub and Bitbucket Integration\nDeployment Keys and User Keys\nMy Pull Request to update this section\n\n\nDeploying documentation to GitHub Pages with continuous integration\nHow to push a commit back to the same repository as part of the CircleCI job\n\nMy previous posts on CircleCI\nCircleCI for R: Motivation and basic setup\nCircleCI for R: Workflows, caches, and more\n\n\nVersion control purists will object to\nsaving generated content in a version control repository. And sure, if you are a\npart of a team working on a large enterprise codebase, you would be wise to save\ngenerated content on a separate server for sharing with others. But for\nindividuals or small teams with fewer resources, saving generated content in the\noriginal Git repository is a practical choice with plenty of benefits: keep\neverything in one place, serve HTML files with GitHub Pages, record history of\nchanges, etc.↩\nWARNING:\nDo not be tempted to use your existing SSH keys. If an SSH key linked to your\nGitHub account is exposed, an attacker would have access to all of your public\nand private repositories.↩\nThe\ncomment passed to -C can be whatever you like. It’s purpose is to remind you\nwhat the key is for.↩\nIn the screenshot below, you can see that I named it\n“CircleCI User Key”. This is because at the time I was confused by the CircleCI\ndocumentation (which I have since fixed). It is a deploy key\nsince it is associated with a single repository. A user key is one that is\nassoiciated with a user account.↩\nFor convenience, I recommend the\nFirefox addon Open CircleCI\nWorkflows.\nWhenever you have a GitHub repository open in your browser, it inserts a button\nwith the CircleCI logo into the address bar. Clicking on this button takes you\ndirectly to the project on CircleCI.↩\nUpdate:\nI had originally said that you could put whatever you like in the Hostname field\nto remind you that this key is for pushing to GitHub. However,\nSampsonM kindly took the time to inform\nme that it actually\nhas to be “github.com”, and I confirmed this is what the CircleCI\ndocs\nstate.↩\nBy default it is in detached HEAD state.↩\nThis can be a security\nconcern. If you’re concerned, e.g. you are pushing sensitive data\nto a private repository, you can confirm the fingerprint in the CircleCI build\nlog matches the fingerprint published in GitHub’s SSH help page.↩\n",
    "preview": {},
    "last_modified": "2023-01-18T16:46:24+00:00",
    "input_file": {}
  },
  {
    "path": "posts/debug-circleci/",
    "title": "Debug your R package on CircleCI via Docker or SSH",
    "description": "If your CircleCI build is failing, you can interactively debug it locally or\nremotely.",
    "author": [
      {
        "name": "John Blischak",
        "url": "https://jdblischak.com"
      }
    ],
    "date": "2019-05-18",
    "categories": [
      "Software Development with R",
      "devops",
      "debugging"
    ],
    "contents": "\nTable of Contents\nDebug locally with Docker\nDebug remotely with SSH\nConclusion\nIf you’re testing your R package on CircleCI (see my previous posts on the basic setup and more advanced configuration), you’ll inevitably need to debug some issue with the build process. I have previously explained how to debug your R package on Travis CI via SSH and on AppVeyor via Remote Desktop. Fortunately debugging is much easier with CircleCI.\nDebug locally with Docker\nIf you have Docker installed on your local machine, you can recreate the exact environment locally. Run the command below to enter a Docker container that has your R package copied to the directory /root/project/ (this is the directory that CircleCI uses by default):\n\ndocker run --rm -it -v <absolute path to R pkg>:/root/project/ rocker/verse:3.6.0 bash\nMake sure to use an absolute path to your R package and also to specify bash as the command to execute (otherwise it starts R, and some other rocker containers start RStudio Server).\nOnce you’ve entered the running Docker container, you first need to run cd /root/project/ to enter your Git repository. Note that these files are not copies; they are the files from your local machine mounted as a volume into the Docker container. Thus any changes you make will be reflected in the local file system.\nFrom here you can recreate the build process. If your build steps are contained in bash scripts, you can run them one by one. Alternatively you can copy-paste from your .circleci/config.yml file until you reach the step that needs to be debugged interactively.\nLastly, in case you need extra software for debugging that you don’t have installed for the standard build, you can install these with APT1. For example, the lines below will install the nano text editor.\n\napt update\napt install -y nano\nDebug remotely with SSH\nIf you don’t have Docker installed on your local machine, it is still convenient to debug CircleCI remotely. Unlike for Travis CI, it is completely secure. Unlike AppVeyor, it is easy to setup.\nFirst, login to CircleCI using your GitHub or Bitbucket account. Then navigate to the failing job and select the dropdown arrow in the top right to click “Rerun job with SSH”.\nCircleCI: Rerun job with SSHSecond, after it spins up the container, it will enable SSH. It is completely secure because for authentication it uses the public SSH key(s) that you’ve already uploaded to GitHub or Bitbucket.\nCircleCI: SSH is enabledFurthermore, when you connect via SSH, you can confirm that the RSA key fingerprint matches what CircleCI listed.\n\n$ ssh -p 64535 3.81.151.107\nThe authenticity of host '[3.81.151.107]:64535 ([3.81.151.107]:64535)' can't be established.\nRSA key fingerprint is SHA256:qNSPQ4R8ARdsnB1LKqUKuVEZnvMrCQCSpKqxYaHte1g.\nAre you sure you want to continue connecting (yes/no)? yes\nWarning: Permanently added '[3.81.151.107]:64535' (RSA) to the list of known hosts.\nAnd once you’ve logged in, CircleCI updates this activity in real time, so you could see if someone else was accessing your build.\nCircleCI: SSH is enabledAfter SSH is enabled, the build continues. Thus you can simply wait until the build progresses to the current error that needs to be debugged, and everything will be setup for you. As described above, you’ll need to navigate to your repository on the remote machine:\n\ncd /root/project/\nLastly, you should have plenty of time for debugging because it will stay connected for up to two hours as long as you are logged in.\nCircleCI: SSH is enabledConclusion\nAnother benefit of using CircleCI is that its interactive debugging is straightforward and secure. No tokens, exposed secrets, or complex setup steps.\nHow could this debugging guide be improved? I assume it’s possible to run RStudio Server and access it via your browser2. If you know how to do this, please let me know and I’ll add the instructions to this post.\nHappy debugging!\nAssuming you’re using one of the rocker Docker images, which use Debian.↩︎\nUnless CircleCI’s servers block port 8787.↩︎\n",
    "preview": {},
    "last_modified": "2023-01-18T16:46:24+00:00",
    "input_file": {}
  },
  {
    "path": "posts/circleci-for-r-2/",
    "title": "CircleCI for R: Workflows, caches, and more",
    "description": "Enhance your continuous integration tests on CircleCI",
    "author": [
      {
        "name": "John Blischak",
        "url": "https://jdblischak.com"
      }
    ],
    "date": "2019-05-17",
    "categories": [
      "Software Development with R",
      "devops"
    ],
    "contents": "\nTable of Contents\nTest multiple versions of R with workflows\nCache the R packages\nCalculate code coverage with covr and Codecov\nAdd a status badge to your README file\nConclusion\nIn my previous post on using CircleCI for R, I provided a basic setup to start testing an R package. Now I’ll discuss how to customize the configuration, specifically the following topics:\nTest multiple versions of R with workflows\nCache the R packages\nCalculate code coverage with covr and Codecov\nAdd a status badge to your README file\nTest multiple versions of R with workflows\nIn the basic setup from the previous post, the version of R was specified by the tag of the Docker image on DockerHub:\n\njobs:\n  build:\n    docker:\n      - image: rocker/verse:3.5.3\nUnfortunately it’s not possible to simply list multiple Docker images and then have the same steps performed in each image. This behavior is possible using a build matrix in Travis CI or AppVeyor. Instead, each separate job has to list its own steps. This is more verbose, but using CircleCI Workflows provides much more flexibility. For example, you can have jobs depend on other jobs and even share files across jobs.\nTo demonstrate, the configuration file below tests an R package using the both the release version of R (currently 3.6.0) and the development version of R.\n\nversion: 2\njobs:\n  release:\n    docker:\n      - image: rocker/verse:3.6.0\n    steps:\n      - checkout\n      - run:\n          name: Install package dependencies\n          command: R -e \"devtools::install_deps(dep = TRUE)\"\n      - run:\n          name: Build package\n          command: R CMD build .\n      - run:\n          name: Check package\n          command: R CMD check *tar.gz\n  devel:\n    docker:\n      - image: rocker/verse:devel\n    steps:\n      - checkout\n      - run:\n          name: Install package dependencies\n          command: R -e \"devtools::install_deps(dep = TRUE)\"\n      - run:\n          name: Build package\n          command: R CMD build .\n      - run:\n          name: Check package\n          command: R CMD check *tar.gz\nworkflows:\n  version: 2\n  all:\n    jobs:\n      - release\n      - devel\nThis involves a lot of repeated code, which makes it more difficult to update (e.g. if you wanted to add the flag --as-cran to R CMD check for all the jobs). One option is to save each step in its own script inside of the .circleci directory, and then have each of the jobs execute each file for a given step. This allows you to update the code in one place and affect every job. This has the added benefit of making it easier to debug your code interactively because you can quickly run the same scripts in a local Docker container. This is the strategy I use in the config.yml for testing my R package workflowr.\nA second option is to use YAML anchor syntax. This allows you to specify YAML entries that can be reused throughout the file. For example, the file below runs the same steps in three different Docker images while only writing the steps in one location:\n\nversion: 2\n\nsteps: &steps\n  steps:\n    - checkout\n    - run:\n        name: Install package dependencies\n        command: R -e \"devtools::install_deps(dep = TRUE)\"\n    - run:\n        name: Build package\n        command: R CMD build .\n    - run:\n        name: Check package\n        command: R CMD check *tar.gz\n\njobs:\n  release:\n    docker:\n      - image: rocker/verse:3.6.0\n    <<: *steps\n  oldrel:\n    docker:\n      - image: rocker/verse:3.5.3\n    <<: *steps\n  devel:\n    docker:\n      - image: rocker/verse:devel\n    <<: *steps\n\nworkflows:\n  version: 2\n  all:\n    jobs:\n      - release\n      - devel\n      - oldrel\nYou can see this approach used by the pyodide project and also in my example repository ci4r.\nCache the R packages\nA large fraction of a job’s build time is dedicated to installing package dependencies, especially on Linux which always build packages from source. Thus a guaranteed way to reduce build times is to cache the R packages.\nBecause CircleCI Workflows allow arbitrarily complex build pipelines, CircleCI also provides a complex caching mechanism. If you only have one job, I recommend ignoring this complexity entirely. To cache the R packages, add the following steps below. The restore_cache needs to before the installation step and the save_cache step after it. The nice thing is that since devtools::install_deps() always installs updated versions if available, you don’t have to worry about the packages in the cache becoming outdated.\n\n    steps:\n      - restore_cache:\n          keys:\n            - cache\n\n      - save_cache:\n          key: cache\n          paths:\n            - \"/usr/local/lib/R/site-library\"\nHowever, this strategy won’t work if you are building for different versions of R since these will require their own separate caches. Instead, you can create a separate cache for each job by adding the job name to the cache key.\n\n    steps:\n      - restore_cache:\n          keys:\n            - cache-{{ .Environment.CIRCLE_JOB }}\n\n      - save_cache:\n          key: cache-{{ .Environment.CIRCLE_JOB }}\n          paths:\n            - \"/usr/local/lib/R/site-library\"\nLastly, if you wanted the cache to be invalidated every time a specific file is updated, e.g. DESCRIPTION, you could use the following cache key:\n\n    steps:\n      - restore_cache:\n          keys:\n            - cache-{{ .Environment.CIRCLE_JOB }}-{{ checksum \"DESCRIPTION\" }}\n\n      - save_cache:\n          key: cache-{{ .Environment.CIRCLE_JOB }}-{{ checksum \"DESCRIPTION\" }}\n          paths:\n            - \"/usr/local/lib/R/site-library\"\nCalculate code coverage with covr and Codecov\nTo calculate the code coverage of your tests with covr and Codecov, add the following step to one of the jobs:\n\n      - run:\n          name: Calculate code coverage\n          command: r -e 'covr::codecov()'\nFortunately, CircleCI doesn’t require any extra configuration! See the covr docs on using Codecov if you haven’t used this service before.\nAdd a status badge to your README file\nOnce you’ve started testing your R package on CircleCI, you’ll want to share its current status in the README file. In the Project Settings, there is a section named Status Badges. This let’s you choose the output format and, if desired, a specific branch. Most likely you’ll want Markdown format for the master branch, which is shown below:\n\n[![CircleCI](https://circleci.com/gh/<username>/<repository>/tree/master.svg?style=svg)](https://circleci.com/gh/<username>/<repository>/tree/master)\nCopy-paste this to the top of your README.md file to start sharing.\nConclusion\nIn this post, I detailed how to test multiple versions of R with workflows, cache the R packages, calculate code coverage with covr and Codecov, and add a status badge to your README file. However, using CircleCI Workflows, it’s possible to create much more complex testing pipelines if your package needs it. Is there any additional task you’d like to know how to accomplish using CircleCI?\nIf you’re interested in how best to use CircleCI for testing R packages, I have an open Pull Request to add a function use_circleci() to the usethis package. I’d appreciate any feedback on the default setup this function provides.\n\n\n",
    "preview": {},
    "last_modified": "2023-01-18T16:46:24+00:00",
    "input_file": {}
  },
  {
    "path": "posts/debug-appveyor/",
    "title": "Debug your R package on AppVeyor via Remote Desktop",
    "description": "If you can't replicate the error locally, you can access an AppVeyor server\nvia Remote Desktop to interactively debug your R package.",
    "author": [
      {
        "name": "John Blischak",
        "url": "https://jdblischak.com"
      }
    ],
    "date": "2019-04-04",
    "categories": [
      "Software Development with R",
      "devops",
      "debugging"
    ],
    "contents": "\nTable of Contents\nInstall Remote Desktop\nConfigure to debug on failure\nSet a password\nLogin to AppVeyor server via Remote Desktop\nNavigating the file system with PowerShell\nSetup for R debugging\nInstall system packages with Chocolatey\nDebugging step-by-step\nResuming the build\nTroubleshooting\nConclusion\n\n\nThe continuous integration service AppVeyor is great for regularly testing your software on Windows, and it is well-supported for R package developers via the r-appveyor project. Run usethis::use_appveyor() to add Windows testing to your R package.\nEven though I have access to a local Windows machine, there have still been times when I couldn’t replicate an AppVeyor error. The cause of hard-to-replicate bugs could be due to differences in the installed software versions (e.g. pandoc, which was the source of my most recent AppVeyor bug) or differences between the desktop and server versions of Windows. The good news is that it is possible to interactively debug an AppVeyor build using Remote Desktop1 In this post, I provide step-by-step instructions on how to setup and debug your R package build on AppVeyor using Remote Desktop.\nInstall Remote Desktop\nWindows: If you are using a Windows machine, you already have Remote Desktop installed. You can either type “Remote Desktop” into the search bar or navigate to Programs -> Windows Accessories -> Remote Desktop Connection.\nmacOS: Follow the instructions in Get started with Remote Desktop on Mac to install Microsoft Remote Desktop 10 from the Mac App Store. Note that this requires macOS 10.11 or later.\nLinux: Use your distribution’s package manager to install Remmina, an open source implementation of Remote Desktop.\nI’ve only ever tested this on Windows and Ubuntu. If you configure Remote Desktop on macOS or another Linux distribution and have tips to share, please open an Issue and I’ll update the post.\nConfigure to debug on failure\nTo have AppVeyor open a connection for you to login and debug your build, add the following line to the on_failure section of appveyor.yml:\n\nps: $blockRdp = $true; iex ((new-object net.webclient).DownloadString('https://raw.githubusercontent.com/appveyor/ci/master/scripts/enable-rdp.ps1'))\nIf there is any error during the installation or checking of your R package, this line tells AppVeyor to pause the build and print the server details for you to connect and debug. If you’re using the default r-appveyor setup, your on_failure step will look like the following:\n\non_failure:\n  - 7z a failure.zip *.Rcheck\\*\n  - appveyor PushArtifact failure.zip\n  - ps: $blockRdp = $true; iex ((new-object net.webclient).DownloadString('https://raw.githubusercontent.com/appveyor/ci/master/scripts/enable-rdp.ps1'))\nThese commands zip the *.Rcheck/ directory, upload it as a build artifact for you to download, and then pause for debugging.\nSet a password\nBefore you commit and push the update to appveyor.yml, it is highly recommended that you first set a password. If you don’t set a password, the default behavior is to print an automatically generated password to the build log. If you have any secrets that are used during your build (e.g. deploy SSH keys, access tokens, etc.), anyone that sees the build log would be able to access this information.2 Your secrets would be vulnerable every time your build failed for up to an hour (since that is the time limit for open source builds on AppVeyor).\nTo set a password using the AppVeyor user interface, click on your project, then go to Settings -> Environment -> Add variable. Name the environment variable APPVEYOR_RDP_PASSWORD. Make sure your password meets the strict minimal requirements that Windows Server enforces for all passwords (see the AppVeyor documentation).\nLogin to AppVeyor server via Remote Desktop\nOnce you push the update and your package build fails,3 AppVeyor will print the connection details in the build log. It should look something like the following:\n\n$blockRdp = $true; iex ((new-object net.webclient).DownloadString('https://raw.githubusercontent.com/appveyor/ci/master/scripts/enable-rdp.ps1'))\nRemote Desktop connection details:\n  Server: 67.225.165.193:33830\n  Username: appveyor\nBuild paused. To resume it, open a RDP session to delete 'Delete me to continue build.txt' file on Desktop.\nThe server address will be different, but the username is always appveyor.\nAfter you open Remote Desktop, click on “Show Options”:\n\nThen enter your server address and appveyor for the username:\n\nNext enter the password you had set as an environment variable:\n\nYou may get a warning about the server’s certificate, but you can safely ignore this and click on Yes to proceed:\n\nIf all goes well, you will now have access to the remote desktop!\n\nIf all did not go well, see the section Troubleshooting below.\n\nTo start debugging, click on the PowerShell icon  in the menu bar.\n\nNavigating the file system with PowerShell\nMicrosoft’s PowerShell is quite different from a Unix shell. Fortunately, many of the commands for navigating the file system are similar.\npwd to print the current working directory\ncd to change directory\nls to list files and directories\nIf you aren’t familiar with Windows filepaths, note that the root of the file system is the Windows drive (in this case C:), paths are separated with \\, and the names are case-insensitive.\nThe session below navigates from the home directory (C:\\Users\\appveyor) to the cloned Git repository (debugAppveyor).\n\nPS C:\\Users\\appveyor> pwd\n\nPath\n----\nC:\\Users\\appveyor\n\n\nPS C:\\Users\\appveyor> cd C:\\projects\\debugappveyor\nPS C:\\projects\\debugappveyor> ls\n\n\n    Directory: C:\\projects\\debugappveyor\n\n\nMode                LastWriteTime         Length Name\n----                -------------         ------ ----\nd-----         4/2/2019   8:22 PM                scripts\nd-----         4/2/2019   8:22 PM                tests\n-a----         4/2/2019   8:22 PM             40 .gitignore\n-a----         4/2/2019   8:22 PM             77 .Rbuildignore\n-a----         4/2/2019   8:22 PM           1158 appveyor.yml\n-a----         4/2/2019   8:22 PM            356 debugAppveyor.Rproj\n-a----         4/2/2019   8:22 PM            412 DESCRIPTION\n-a----         4/2/2019   8:22 PM             22 failure.zip\n-a----         4/2/2019   8:22 PM             31 NAMESPACE\n-a----         4/2/2019   8:22 PM            202 README.md\n-a----         4/2/2019   8:22 PM             32 travis-tool.sh.cmd\nSetup for R debugging\nUnfortunately you cannot immediately start debugging. First you have to setup the necessary environment variables to be able to access the R executable and packages that were installed via the steps in appveyor.yml.\nR is installed at C:\\R. Add it to your path by running4:\n\n$env:Path = \"C:\\R\\bin;\" + $env:Path\nYou should now be able to run R.exe to start the R console or Rscript.exe. If this isn’t working, run $env:Path to confirm that C:\\R\\bin was added to the beginning and ls C:\\R\\bin to confirm that R has been installed.\nThe R packages were installed in C:\\Rlibrary. Set this as your R user library with:\n\n$env:R_LIBS_USER = \"C:\\Rlibrary\"\nAnd confirm that Rscript.exe -e \".libPaths()\" returns [1] \"C:/RLibrary\"  \"C:/R/library\".\nNow you can start debugging! Run R.exe to open the R console.5 You can install R packages with install.packages() as you normally would. You can build and test the package using devtools functions (e.g. install(), build(), test() or check()) or using R.exe directly:\n\nR.exe CMD build .\nR.exe CMD check *tar.gz \nIf you need to edit a file, I recommend using Notepad++, since this will preserve the line breaks and provide some basic syntax highlighting. You can either open it through the Windows start menu or alternatively open files from R using file.edit() after setting the R option with\n\noptions(editor = \"C:/Program Files (x86)/Notepad++/notepad++.exe\")\nUpdate: And if you need some more serious debugging tools, you can install RStudio (thanks to Jim Hester for the tip!). You can open Internet Explorer, download the latest version of RStudio Desktop for Windows at https://www.rstudio.com/products/rstudio/download, and run the installer.\nAnd if you find yourself doing this often, you can automate this using the following PowerShell script:6\n\nInvoke-WebRequest https://download1.rstudio.org/desktop/windows/RStudio-1.2.5033.exe -OutFile rstudio.exe\nStart-Process -FilePath .\\rstudio.exe -ArgumentList \"/S /v/qn\" -Wait\nStart-Process 'C:\\Program Files\\RStudio\\bin\\rstudio.exe'\nI figured out how to install RStudio with PowerShell from this script from Tomaž Kaštrun. To learn more about automating installation steps with PowerShell, check out his blog post Installing R using Powershell.\nInstall system packages with Chocolatey\nIf you need to install different versions of non-R dependencies, you can use the package manager Chocolatey. Below is a quick primer to demonstrate how to install, list, and uninstall.\n\n# Install latest version of pandoc\nchoco install pandoc\n# View all available versions of pandoc\nchoco list pandoc --exact --all\n# Install a previous version of pandoc\nchoco install pandoc --version 1.19.1 --allow-downgrade\n# Uninstall pandoc\nchoco uninstall pandoc\nTo quickly use the installed software, run refreshenv to update the environment variables. Unfortunately this only works for interactive use. If need the software to be available to your R package during the build, you need to manually add it to the path. For example, the configuration below installs pandoc 1.19.1 and adds it to the path.\n\nbefore_test:\n  - ps: choco install pandoc --version 1.19.1 --allow-downgrade\n  - ps: $env:Path += \";C:\\Program Files (x86)\\Pandoc\\\"\nDebugging step-by-step\nInstead of starting the debugger after a failure, you can also purposefully start the debugger earlier. This is useful if one of the setup steps is causing the failure. To start the debugger after the repository has been cloned but before any more setup has occurred, update the install step to include the debug line:\n\ninstall:\n  - ps: $blockRdp = $true; iex ((new-object net.webclient).DownloadString('https://raw.githubusercontent.com/appveyor/ci/master/scripts/enable-rdp.ps1'))\n  - ps: Bootstrap\nNow when you login, R is not even installed yet. To run each step individually, navigate to your directory in C:\\projects, load the functions in appveyor-tool.ps1, and confirm the module was successfully loaded by running the Progress function, which prints the provided message along with the current date and time.\n\ncd C:\\projects\\debugappveyor\nImport-Module ..\\appveyor-tool.ps1\nProgress \"Module successfully loaded\"\nNext run Bootstrap to install R and download the travis-tool script. Then you can install the dependencies and run the tests using TravisTool.\n\nBootstrap\nTravisTool install_deps\nTravisTool run_tests\nResuming the build\nTo resume the build, you need to delete the file Delete me to continue build.txt. You can either delete it using the GUI interface or by running:\n\n rm 'C:\\Users\\appveyor\\Desktop\\Delete me to continue build.txt'\nAfter it has been deleted, any remaining steps will be performed and then the connection will close.\nTroubleshooting\nConnection error: If you see a message like the following when you attempt to connect to the AppVeyor server, something is wrong on AppVeyor’s end. I recommend doing something else and returning to your debugging effort later. Even if you manage to log in, the connection can drop at any time if the service is experiencing problems.\n\nCached package error: If the error you are having is caused by a cached package that can’t be updated, you need to invalidate or delete the cache. The easiest method to invalidate the cache is to make it depend on a file. Changing the cache step below will invalidate the cache every time you change appveyor.yml.\n\ncache:\n  - C:\\RLibrary -> appveyor.yml\nAlternatively, you can delete the cache manually in the JavaScript console via the AppVeyor API, but this is a much more involved process, so I recommend using the strategy above instead.\nConclusion\nInteractively debugging an AppVeyor build is an involved process, but it may be the only way to finally determine the problem, especially if you can’t replicate the bug on your local Windows machine or if you don’t have easy access to one. For more information, see the official AppVeyor documentation on Accessing Windows build worker via Remote Desktop. And to practice debugging, check out my repository debugAppveyor. Happy debugging!\nI’ll refer to this software as Remote Desktop throughout this post. It used to be called Remote Desktop Protocol, so the AppVeyor documentation refers to it as RDP. However, Microsoft now calls it Remote Desktop Connection.↩\nThis is similar to how interactive debugging works on Travis CI. Anyone that sees the build log can access the server. The main difference is that a debug build has to be manually triggered via the Travis API, so you at least know every time your secrets are vulnerable. See my previous post for details.↩\nIf you don’t currently have an R package that is failing on AppVeyor (lucky you!), you can try out the steps below by forking my repository debugAppveyor. It already has appveyor.yml setup for debugging and a failing test, so you just have to activate AppVeyor for your forked repository, add a password, and then trigger a new build.↩\nThe easiest method to paste into the PowerShell terminal is to right click. Alternatively, you can click on the icon in the top left of the window to open the menu, and then select Edit -> Paste↩\nEntering only R will re-execute the most recent command.↩\nThis will always install version 1.2.5033, so you may want to update this. Unfortunately it is non-trivial to determine the download URL of the latest RStudio release.↩\n",
    "preview": {},
    "last_modified": "2023-01-18T16:46:24+00:00",
    "input_file": {}
  },
  {
    "path": "posts/debug-travis/",
    "title": "Debug your R package on Travis CI via SSH",
    "description": "If you can't replicate the error locally, you can SSH into a Travis server to\ninteractively debug your R package.",
    "author": [
      {
        "name": "John Blischak",
        "url": "https://jdblischak.com"
      }
    ],
    "date": "2019-03-26",
    "categories": [
      "Software Development with R",
      "devops",
      "debugging"
    ],
    "contents": "\nTable of Contents\ntravis-ci.com vs travis-ci.org\nEmail for debug access\nGet your API token\nFind job ID number\nRestart job in debug mode\nMiscellaneous\nConclusion\nIdeally if R CMD check passes on your local machine, then hopefully it will also pass when run on remote CI servers. But of course this isn’t always the case, and it’s a great way to find bugs (since your users won’t have the exact same setup as your local machine). Often the error messages are sufficient for you to be able to guess at the solution. But if you’ve pushed multiple commits to try and fix the problem, and the CI jobs are still failing, it is probably worth investing the time to interactively debug the job on the remote server. In this post, I’ll describe how to do this for Travis CI. Travis CI documents the process in Running Build in Debug Mode, and below I tailor this specifically for debugging R packages.\ntravis-ci.com vs travis-ci.org\nBefore going any further, you should go check the URL of your Travis CI jobs, because this will affect the steps below. Initially Travis CI hosted open source repositories on travis-ci.org and private repositories on travis-ci.com. However, now they are migrating all projects to use travis-ci.com. Thus if you setup Travis prior to May 2018, you are likely using travis-ci.org; whereas if you set it up post May 2018, you are likely using travis-ci.com. See the documentation Open Source on travis-ci.com for all the details. Currently the migration of open source projects from travis-ci.org to travis-ci.com is in Beta, so I wouldn’t recommend trying to migrate your repository unless you were really motivated to use the new GitHub App version of Travis CI.\nEmail for debug access\nTo start, you’ll need to email Travis CI support at support@travis-ci.com to activate debug mode for your repository. They got back to me in about two hours.\nThe reason that this feature is not automatically enabled is for security reasons. Below is the explanation from the email they sent me that confirmed activation of debug mode:\n\nThe reason this is not enabled by default for public repositories is that anyone who happens to stumble across a log containing the SSH access can make a connection to the VM and potentially read out the secret environment variables.\n\nYou can learn more in the section Security considerations. Briefly, while only you are able to initiate a debugging session, if someone with malicious intent were to be watching that exact build log, they could also run the SSH command to login. This is mainly a concern if you’ve added encrypted environment variables ( e.g. SSH deploy keys or personal access tokens). A standard setup to test an R package, e.g. with usethis::use_travis(), does not involve any sensitive information, so this likely isn’t a concern for you.\nGet your API token\nOnce you’ve received your email confirmation from Travis CI support, you can obtain your authentication token. The token allows only you to trigger a debug build for your repository. Thus do not save this anywhere that others can view. If you’re using travis-ci.com, go to https://travis-ci.com/profile. If you’re using travis-ci.org, go to https://travis-ci.org/account/preferences (screenshot below).\nTravis API token from travis-ci.orgFind job ID number\nNext you need to find the job ID number of the job you want to restart in debug mode. The key is to not use the build ID number. When you first go to the build log, the URL will be https://travis-ci.<org|com>/<username>/<repository>/builds/<build ID>. When you click on the individual job that you want to debug, the URL will change to https://travis-ci.<org|com>/<username>/<repository>/jobs/<job ID>. Copy-paste the job ID. This isn’t sensitive information (because it’s already public), so you don’t need to be careful about it. If you only have one job, then click on either “Job log” or “View config” to change the URL.\nRestart job in debug mode\nWith both the authorization token and job ID, you can trigger the Travis API to restart the job in debug mode. Specifically you will use the debug endpoint of the Travis API v3.1\nOpen a terminal and copy-paste the following API call, replacing the placeholders with your actual API token and job ID. Also make sure to change the URL to travis-ci.com if that is what you are using.\n\n$ curl -s -X POST \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Accept: application/json\" \\\n  -H \"Travis-API-Version: 3\" \\\n  -H \"Authorization: token <API token>\" \\\n  -d \"{\\\"quiet\\\": true}\" \\\n  https://api.travis-ci.org/job/<job ID>/debug\nIf this is successful, it will return a JSON blob that looks like the following:\n\n{\n  \"@type\": \"pending\",\n  \"job\": {\n    \"@type\": \"job\",\n    \"@href\": \"/job/<job ID>\",\n    \"@representation\": \"minimal\",\n    \"id\": <job ID>\n  },\n  \"state_change\": \"created\",\n  \"resource_type\": \"job\"\n}\nNow go to the page of the job you just restarted. Recall that the URL will look like https://travis-ci.<org|com>/<username>/<repository>/jobs/<job ID>. Once the job starts, a few steps will be done prior to providing you SSH access:\nInstall R\nClone your Git repository\nSet any environment variables defined in .travis.yml or via the Travis Settings\nRestore cached packages\nNote that the last item could be a potential problem. If you suspect your errors may be due to an outdated dependency, you will want to first clear the package cache by clicking on “More options” -> “Caches” and deleting the relevant cache.\nAfter these steps, the following message will be printed in the build log:\n\nDebug build initiated by <username>\nrm: cannot remove ‘/home/travis/.netrc’: No such file or directory\nSetting up debug tools.\nPreparing debug sessions.\nUse the following SSH command to access the interactive debugging environment:\nssh <random string>@to2.tmate.io\nThis build is running in quiet mode. No session output will be displayed.\nCopy-paste the ssh command into your terminal. Note that you’ll likely receive a warning about the authenticity of the host, which you can accept:\n\n$ ssh RXkB2WDHNOeOAdYXumNzNU47f@to2.tmate.io\nThe authenticity of host 'to2.tmate.io (159.203.36.122)' can't be established.\nECDSA key fingerprint is SHA256:8GmKHYHEJ6n0TEdciHeEGkKOigQfCFuBULdt6vZIhDc.\nAre you sure you want to continue connecting (yes/no)? yes\nThis will drop you into a tmate2 session. Note that the tmate terminal behaves a little differently than a typical terminal. You can move up and down the command history with the up and down arrows, but if you want to do anything more complicated, e.g. reverse search, you’ll need to first invest some time in learning the tmux key bindings.\nNow you can start interactively debugging your package! The next steps will be specific to your package and the given error. If one of your dependencies is failing to install, you can open R and try to install a previous version. Or you could try installing some system libraries with APT. If the error is in the check step, you can run the installation step with the convenience function travis_run_install. There are convenience functions for each of the potential steps in .travis.yml, and you can see the default steps in the R-specific build documentation.\nIf you know that R CMD check results in a warning or an error, do not run travis_run_script. The SSH session will automatically terminate if there is an error. Instead you can run devtools::install() to install your R package and then interactively test it. If you’re using testthat, you can run devtools::test(filter = \"<string>\") to run the specific tests that are failing.\nTo give you a sense of how much control you have, the last time I had to interactively debug my R package, I suspected that pandoc 2 was causing the issue. Thus after running devtools::test() to confirm the error, I installed pandoc 1.19.2.13 and then reran devtools::test() to confirm the error was fixed.\n\nsudo apt remove pandoc\nwget https://github.com/jgm/pandoc/releases/download/1.19.2.1/pandoc-1.19.2.1-1-amd64.deb\nsudo dpkg -i pandoc-1.19.2.1-1-amd64.deb \nMiscellaneous\nA few final notes that may be helpful as you debug your Travis build:\nThe working directory when you login is /home/travis/build/<username>/<repository>\nThe home directory is /home/travis\nThe debugging session will automatically end after 50 minutes, which is the time limit for open source jobs on Travis CI\nConclusion\nWith any luck you won’t have to interactively debug your R package directly on a Travis server. But if you can’t replicate the error locally, using debug mode is much easier than debugging by the method of guess-push-hope-repeat.\nDo you have any tips for debugging an R package on Travis? Any harrowing debugging adventures to share?\nNote that this viewing the API documentation requires you to login.↩\nA fork of tmux↩\nThis was the latest of the 1.x series that had a deb file available.↩\n",
    "preview": {},
    "last_modified": "2023-01-18T16:46:24+00:00",
    "input_file": {}
  },
  {
    "path": "posts/circleci-for-r-1/",
    "title": "CircleCI for R: Motivation and basic setup",
    "description": "If you like using Docker, you should try out CircleCI for testing your\nR packages and projects.",
    "author": [
      {
        "name": "John Blischak",
        "url": "https://jdblischak.com"
      }
    ],
    "date": "2019-03-25",
    "categories": [
      "Software Development with R",
      "devops"
    ],
    "contents": "\nTable of Contents\nWhy try CircleCI?\nMinimal setup to check an R package\nCircleCI limitations\nFuture posts\nI really enjoy Continuous Integration (CI) testing. The immediate feedback is super useful, especially when you need to test your code across multiple different setups (e.g. different operating systems or versions of dependencies). The rise of free CI services has hugely impacted my workflow, similar to when I started using version control.\nIf you haven’t started using CI to automatically test your R package(s), you should go run usethis::use_appveyor() and usethis::use_travis(). These will setup Windows testing on AppVeyor and Linux testing on Travis CI, respectively. Since AppVeyor and Travis CI are the best supported CI platforms for R packages, they are definitely where you should start.\nHowever, if you’ve already started using CI services, I want to encourage you to try CircleCI. I recently started using it to test an R package and various analysis projects, and it has a nice feature set that can make it more convenient to customize compared to either AppVeyor or Travis.\nWhy try CircleCI?\nCircleCI has various features that I have found useful:\nSpecify any Docker image on DockerHub for the computational environment. This makes local debugging much easier. If it works in a local Docker container, you know it’ll also work on CirlceCI’s servers. I’m too often surprised by the errors I get from AppVeyor or Travis that I don’t get on my local machines. Note that while you can use Docker with Travis, it’s more cumbersome (you have to prefix all your commands with docker run or similar).\nEasy to upload artifacts. Similar to AppVeyor, it is very easy to specify individual files or entire directories to upload as artifacts. This makes it very convenient to find the error in a log file or inspect the result of the CI build. This is much easier to configure compared to Travis, which requires you to setup an AWS instance to upload artifacts.\nLong run time. Travis builds time out after 50 minutes. AppVeyor builds time out after 1 hour. I have had CircleCI jobs successfully complete after running for over 2 hours. The offical pricing information states that the free plan provides 1,000 build minutes for free, but this only applies to private projects. There is no monthly limit on build minutes for open source projects.\nStraight-forward to setup SSH keys. For my analysis projects, I often want to be able to commit some result back to my GitHub repository. A common technique for authenticating the CI server to push a commit to one of your Git repositories is to use an SSH deploy key. The basic idea is that you upload a public key to the repository, and then you upload the encrypted private key to the CI server. This way if the private key were to be compromised, an attacker would only have access to one repository. I found the Travis instructions for setting up a deploy key to be confusing and unnecessarily tedious (from what I can tell, you have to specify the key by committing an encrypted file to the repository). This is likely because they recommend you use other deployment strategies. In contrast, I found the CircleCI instructions and steps much more straight-forward to follow.\nMinimal setup to check an R package\nNow I’ll demonstrate how you can configure a minimal setup to check an R package on CircleCI. In a future post, I’ll detail some options for customization. The first decision is which Docker image to use. Fortunately for R users, the rocker project provides many Docker images customized for running R. I recommend that you start with the image verse because it contains everything you’ll likely need for checking your package, specifically devtools for installing dependencies and texlive/pandoc for building documentation. You’ll also need to decide whether you want to specify the exact version of R used in the Docker image or to always use the latest version of R available.1 Below I will use the tag 3.5.3 to always use the image with R 3.5.3 installed, the latest release when this post was published.\nNext you need to know what commands to run to build and check the R package. For this you can copy the main steps from the Travis CI R setup:\n\n# Install package dependencies\nR -e \"devtools::install_deps(dep = TRUE)\"\n# Build package\nR CMD build .\n# Check package\nR CMD check *tar.gz\nTo instruct CircleCI to run these commands inside the specified Docker image, you need to translate this to the CircleCI YAML configuration file. Below is a basic template. You specify the Docker image to pull from DockerHub and then specify the steps to run. The first is the keyword checkout, which essentially runs git clone. Then for each custom step, you use the key word run and specify a name and command.\n\nversion: 2\njobs:\n  build:\n    docker:\n      - image: <DockerHub image>\n    steps:\n      - checkout\n      - run:\n          name: <description of this step>\n          command: <code to run>\nThus your minimal setup to build and check an R package looks like this:\n\nversion: 2\njobs:\n  build:\n    docker:\n      - image: rocker/verse:3.5.3\n    steps:\n      - checkout\n      - run:\n          name: Install package dependencies\n          command: R -e \"devtools::install_deps(dep = TRUE)\"\n      - run:\n          name: Build package\n          command: R CMD build .\n      - run:\n          name: Check package\n          command: R CMD check *tar.gz\nWhich you need to save in a file named .circleci/config.yml. Note that you will also need to ignore this directory when building your R package. You can automatically add it to your .Rbuildignore with usethis::use_build_ignore(\".circleci\"). Next, commit the new files and push to the remote repository.\n\ngit add .circleci/ .Rbuildignore\ngit commit -m \"Configure CircleCI\"\ngit push origin master\nGo to https://circleci.com/ and login with your GitHub or Bitbucket credentials. Click on “Add Projects” in the left sidebar. Find your repository in the list and click “Set Up Project”. This will present you with a ton of options. You can ignore all these since you’ve already committed .circleci/config.yml. Scroll to the bottom and click “Start Building”. Note that you’ll receive an email notification stating that a new SSH deploy key has been added to your repository.\nFigure: CircleCI setup iconsNow your build should start. It will perform each step in sequence, and you can click on any of the steps to see its log. Once it finishes, it should look like below:\nCircleCI build stepsAnd now this build will be triggered each time you push a new commit!\nOne last thing for the minimal setup. You can upload the entire results directory by adding the following two lines after the build steps, including the name of your package in the filepath.\n\n      - store_artifacts:\n          path: <insert-name-of-pkg>.Rcheck/\nAfter you commit and push this change to .circleci/config.yml, there will be a new step named “Uploading artifacts”. Once this step has completed, you can click on the tab named “Artifacts” to view any of the files, e.g. 00check.log.\nCircleCI limitations\nEvery CI platform I have used has had its advantages and disadvantages. I hope I’ve convinced you that CircleCI is worth trying, but I also want to highlight some of its limitations.\nFirst, it is not possible to link to a specific line in the build logs. This is really unfortunate, especially when build logs get really long (e.g. lots of package dependencies with compiled code). When using Travis and AppVeyor, I often link to the exact line that contains the relevant error message when creating a GitHub Issue or asking someone else to help troubleshoot.\nSecond, pull requests sent by other users to your repository are not built automatically. This is really unfortunate since one the main advantages of using CI is that it helps you evaluate code that others contribute to your project. To activate this important feature, you have to manually go to Settings -> Advanced Settings and turn on “Build forked pull requests”.\nFuture posts\nThere is a lot more you can do with CircleCI. In future posts, I’ll address dependency caching, adding an SSH deploy key, and testing an analysis project. To learn more about testing R packages on CircleCI, I highly recommend this post by Marek Rogal on the Appsilon blog. You can also check out my example GitHub repository ci4r, which has a small R package that is setup for automatic checking with AppVeyor, CircleCI, and Travis CI.\nThe decision is a tradeoff. Always using the latest version of R ensures that your package is compatible with any new R behavior. This will hopefully catch the error before any of your users, which is one of the main advantages of CI. The downside is that it can make it harder to debug. If you push a new commit to your Git repository and the version of R used for the check has changed, it will be harder to discern if the failure is due to your new change or the change in R version.↩\n",
    "preview": {},
    "last_modified": "2023-01-18T16:46:24+00:00",
    "input_file": {}
  }
]
