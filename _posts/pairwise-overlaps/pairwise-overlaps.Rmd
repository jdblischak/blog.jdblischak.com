---
title: "How to efficiently calculate pairwise overlaps of many sets"
description: |
  An example of progressively optimizing the speed of a function
author:
  - name: John Blischak
    url: https://jdblischak.com
date: 03-27-2020
output:
  distill::distill_article:
    self_contained: false
    toc: true
categories:
  - Software Development with R
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

For one of my projects, I needed to calculate the pairwise overlaps of many
sets, and my initial brute force approach was unbearably slow. And even after
making some of the more obvious optimizations, it was still quite slow.
Fortunately I got a good tip from [Nir Graham][nirgrahamuk] when I [posted on
RStudio Community][rs-community-post]. Below I explain the problem in more
detail, and then demonstrate how I progressively optimized the function for
increased speed.^[Note that I didn't take these exact steps. Real life is a lot
messier. I purposefully chose to break the process down into the steps below for
didactic purposes.]

The final solution is still brute force, just faster. If you know of a clever
algorithm for finding pairwise overlaps^[For example, this [SO
answer](https://stackoverflow.com/a/27370005) suggests a pseudo-mergesort might
be a potential solution, but doesn't provide an implementation.], please do get
in touch.

## The combinatorial explosion of pairwise overlaps

First, why do I need to do this? I suspect there are many use cases for
computing pairwise overlaps, but in my specific case I wanted to compare the
similarity of biological annotation terms after having performed a [gene set
enrichment
anlaysis](https://en.wikipedia.org/wiki/Gene_set_enrichment_analysis). For those
unfamiliar with ["omics"](https://en.wikipedia.org/wiki/Omics)-style analyses,
the results are usually a list of biological features with varying levels of
statistical signficance (e.g. which proteins change in relative abundance
between control versus treated cells). To help interpretation, the list of
results is compared to curated databases of annotation terms such as [Gene
Ontology categories](https://en.wikipedia.org/wiki/Gene_ontology) or [KEGG
pathways](https://en.wikipedia.org/wiki/KEGG). While they are usually more
sophisticated, conceptually you can imagine a [chi-squared
test](https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test) of a
contingency table comparing the features in the results list with the features
in each annotation category:

Test | Feature in results | Feature *not* in results
------------- | ------------- | -------------
**Feature in annotation**  | x | y
**Feature *not* in annotation**  | z | w

If a given annotation category contains more of the results than would be expected
by chance, then this category is said to be "enriched".

A major hurdle for interpreting enrichment results is that the annotation categories
often contain many of the same features, and thus the results can be very
redundant. For example, how different are the categories [cellular response to
stress](https://www.ebi.ac.uk/QuickGO/term/GO:0033554) and [response to
stress](https://www.ebi.ac.uk/QuickGO/term/GO:0006950)? Thus it would be nice to
have a metric of how related the categories are when interpeting the enrichment
results.

Specifically I'd like to calculate three measurements:

1. The number of overlapping features
1. The [overlap coefficient](https://en.wikipedia.org/wiki/Overlap_coefficient):
$\frac{| X \cap Y |}{min(|X|, |Y|)}$
1. The [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index):
$\frac{| X \cap Y |}{| X \cup Y |}$

This will be especially useful for a network visualization of the enrichment
results, where the color of the nodes could correspond to the statistical
significance, and the thickness of the eges could correspond to the Jaccard
index. For example, the hypothetical results below demonstrate that there are
two main sources of enrichment, even though five categories are statistically
significant:

```{r network-viz, echo=FALSE}
nodes <- data.frame(
  name = c("one", "two", "three", "four", "five"),
  significance = c(0.1, 0.25, 0.15, 0.001, 0.003),
  stringsAsFactors = FALSE
)
edges <- data.frame(
  from = c("one", "one", "two", "four"),
  to  = c("two", "three", "three", "five"),
  jaccard = c(0.4, 0.45, 0.48, 0.9),
  stringsAsFactors = FALSE
)
network <- igraph::graph_from_data_frame(d = edges, directed = FALSE,
                                         vertices = nodes)
# plot(network)
library(ggraph)
library(tidygraph)

graph <- as_tbl_graph(network, directed = FALSE)

# plot using ggraph
ggraph(graph, layout = "kk") +
  geom_edge_fan(aes(width = jaccard), show.legend = FALSE) +
  geom_node_point(aes(color = significance), size = 5, show.legend = FALSE) +
  geom_node_text(aes(label = name), nudge_y = 0.2) +
  scale_edge_width_continuous(range = c(0.25, 2)) +
  theme_graph()
```

The essential problem that this post deals with is the fact that as the number
of sets increases, the number of pairwise overlaps to be calculated explodes.
100 sets require `r format(choose(100, 2), big.mark = ",")` pairwise overlaps,
but 1000 sets require `r format(choose(1000, 2), big.mark = ",")`. To be more
precise, this is a [combinatorial
relationship](https://en.wikipedia.org/wiki/Combination), in which the number of
pairwise overlaps is determined by the number of unique combinations of size 2
of the $n$ sets:

$$ \binom{n}{2} = \frac {n!} {2!(n-2)!} $$
In R, I'll use the function `choose()` to perform this calculation.

Below is a visualization of the combinatorial explosion in the number of
pairwise overlaps:

```{r combinatorial-explosion, echo=FALSE, preview=TRUE}
suppressPackageStartupMessages(library(ggplot2))
d <- data.frame(x = seq(1, 2500, by = 50),
                y = choose(n = seq(1, 2500, by = 50), k = 2))
ggplot(d) +
  aes(x = x, y = y) +
  geom_point() +
  labs(x = "Number of sets", y = "Number of\npairwise overlaps",
       title = "Combinatorial explosion") +
  scale_y_continuous(labels = scales::label_comma()) +
  theme_classic() +
  theme(axis.title.y = element_text(angle = 0, vjust = 0.5))
```

In my application, I have multiple biological annotations that have thousands of
sets to compare. Thus I need the overlap calculations to be quick since they'll
be performed millions of times.

## Simulating sets

To benchmark my implementations, I created the function `simulate_sets()` to
create a list with a given number of elements containing character vectors.

```{r simulate-sets}
simulate_sets <- function(n_sets) {
  universe <- c(letters, LETTERS)
  sets <- replicate(n_sets,
                    sample(universe, size = rpois(1, lambda = 15)),
                    simplify = FALSE)
  names(sets) <- paste0("set", seq(n_sets))
  return(sets)
}
```

The elements in the sets consist of lowercase and uppercase letters. Here's what
the output looks like:

```{r simulate-sets-ex}
set.seed(12345)
(sets_5 <- simulate_sets(n_sets = 5))
sets_100 <- simulate_sets(n_sets = 100)
sets_1000 <- simulate_sets(n_sets = 1000)
```

## First pass implementation

For my first pass, I use a nested for loop to iterate over the names of the
lists. The main advantages of this approach is that it was quick to write and it
is straightforward to read the code. The essential part of this strategy is
contained in the snippet below:

```{r eval=FALSE}
for (name1 in names(sets)) {
  set1 <- sets[[name1]]
  for (name2 in names(sets)) {
    set2 <- sets[[name2]]
  }
}
```

Here's the entire function:

```{r}
calc_pairwise_overlaps <- function(sets) {

  vec_name1 <- character()
  vec_name2 <- character()
  vec_num_shared <- integer()
  vec_overlap <- numeric()
  vec_jaccard <- numeric()

  for (name1 in names(sets)) {
    set1 <- sets[[name1]]
    for (name2 in names(sets)) {
      set2 <- sets[[name2]]
      set_intersection <- intersect(set1, set2)
      num_shared <- length(set_intersection)
      overlap <- num_shared / min(length(set1), length(set2))
      jaccard <- num_shared / length(union(set1, set2))

      vec_name1 <- c(vec_name1, name1)
      vec_name2 <- c(vec_name2, name2)
      vec_num_shared <- c(vec_num_shared, num_shared)
      vec_overlap <- c(vec_overlap, overlap)
      vec_jaccard <- c(vec_jaccard, jaccard)
    }
  }

  result <- data.frame(name1 = vec_name1,
                       name2 = vec_name2,
                       num_shared = vec_num_shared,
                       overlap = vec_overlap,
                       jaccard = vec_overlap,
                       stringsAsFactors = FALSE)
  return(result)
}
```

There are of course lots of things to be improved, as you'll see below, but if
you have 100 sets or fewer, it probably wouldn't be worth the effort to optimize
it further.

```{r}
head(calc_pairwise_overlaps(sets_100))
system.time(calc_pairwise_overlaps(sets_100))
```

## Combinations and indexing

One big issue with the strategy above is that each pairwise combination is
performed twice. In other words, I calculated the overlap statistics for each
permutation of the sets, when I really only need to calculate each combination.
The second issue I'll address in this iteration is how to fetch each set. Above
I obtained each set by name, `sets[[name1]]`, which requires searching over the
names of the list to find a match. While convenient, this gets slower as the
list grows. Thus instead of looping over the names of the list, it is better to
use an index variable to track the position in the list. This allows the sets
to be quickly obtained via their position in the list.

The essential part of this update is in the snippet below. The index variables
`i` and `j` are used to obtain the name of the sets and their contents in each
iteration. Furthermore, `j` is always greater than `i`, to avoid comparing a set
to itself or repeating any previous combinations.

```{r eval=FALSE}
n_sets <- length(sets)
set_names <- names(sets)
for (i in seq_len(n_sets - 1)) {
  name1 <- set_names[i]
  set1 <- sets[[i]]
  for (j in seq(i + 1, n_sets)) {
    name2 <- set_names[j]
    set2 <- sets[[j]]
  }
}
```

Here's the updated function:

```{r}
calc_pairwise_overlaps <- function(sets) {

  n_sets <- length(sets)
  set_names <- names(sets)

  vec_name1 <- character()
  vec_name2 <- character()
  vec_num_shared <- integer()
  vec_overlap <- numeric()
  vec_jaccard <- numeric()

  for (i in seq_len(n_sets - 1)) {
    name1 <- set_names[i]
    set1 <- sets[[i]]
    for (j in seq(i + 1, n_sets)) {
      name2 <- set_names[j]
      set2 <- sets[[j]]
      set_intersection <- intersect(set1, set2)
      num_shared <- length(set_intersection)
      overlap <- num_shared / min(length(set1), length(set2))
      jaccard <- num_shared / length(union(set1, set2))

      vec_name1 <- c(vec_name1, name1)
      vec_name2 <- c(vec_name2, name2)
      vec_num_shared <- c(vec_num_shared, num_shared)
      vec_overlap <- c(vec_overlap, overlap)
      vec_jaccard <- c(vec_jaccard, jaccard)
    }
  }

  result <- data.frame(name1 = vec_name1,
                       name2 = vec_name2,
                       num_shared = vec_num_shared,
                       overlap = vec_overlap,
                       jaccard = vec_overlap,
                       stringsAsFactors = FALSE)
  return(result)
}
```

With these two improvements, the computation time more than halved for a list of
100 sets. However, it is still quite slow when running 1000 sets.

```{r}
system.time(calc_pairwise_overlaps(sets_100))
```

And these improvements are essentially the advice from this [StackOverflow answer](https://stackoverflow.com/a/27369862) on the same topic.

## Preallocation

The next issue I'll address is a common cause of slow R code: growing objects
during each iteration of a loop, e.g. `vec_overlap <- c(vec_overlap, overlap)`.
This may be convenient, but it is extremely slow. The solution is to preallocate
the vectors to their final length, and then add new data by index.

In this case, the final length will be the total number of combinations, which
I calculate using the function `choose()`. Furthermore, I add a new index
variable, `overlaps_index`, to keep track of the combinations. The essential
code to preallocate the vectors is in the code snippet below:

```{r eval=FALSE}
# preallocate the vectors
n_overlaps <- choose(n = n_sets, k = 2)
vec_name1 <- character(length = n_overlaps)
vec_name2 <- character(length = n_overlaps)
vec_num_shared <- integer(length = n_overlaps)
vec_overlap <- numeric(length = n_overlaps)
vec_jaccard <- numeric(length = n_overlaps)
overlaps_index <- 1

# During each iteration, use the index to assign the latest value to the output
# vectors
vec_name1[overlaps_index] <- name1
vec_name2[overlaps_index] <- name2
vec_num_shared[overlaps_index] <- num_shared
vec_overlap[overlaps_index] <- overlap
vec_jaccard[overlaps_index] <- jaccard
overlaps_index <- overlaps_index + 1
```

And here's the updated function:

```{r}
calc_pairwise_overlaps <- function(sets) {

  n_sets <- length(sets)
  set_names <- names(sets)
  n_overlaps <- choose(n = n_sets, k = 2)

  vec_name1 <- character(length = n_overlaps)
  vec_name2 <- character(length = n_overlaps)
  vec_num_shared <- integer(length = n_overlaps)
  vec_overlap <- numeric(length = n_overlaps)
  vec_jaccard <- numeric(length = n_overlaps)
  overlaps_index <- 1

  for (i in seq_len(n_sets - 1)) {
    name1 <- set_names[i]
    set1 <- sets[[i]]
    for (j in seq(i + 1, n_sets)) {
      name2 <- set_names[j]
      set2 <- sets[[j]]
      set_intersection <- intersect(set1, set2)
      num_shared <- length(set_intersection)
      overlap <- num_shared / min(length(set1), length(set2))
      jaccard <- num_shared / length(union(set1, set2))

      vec_name1[overlaps_index] <- name1
      vec_name2[overlaps_index] <- name2
      vec_num_shared[overlaps_index] <- num_shared
      vec_overlap[overlaps_index] <- overlap
      vec_jaccard[overlaps_index] <- jaccard

      overlaps_index <- overlaps_index + 1
    }
  }

  result <- data.frame(name1 = vec_name1,
                       name2 = vec_name2,
                       num_shared = vec_num_shared,
                       overlap = vec_overlap,
                       jaccard = vec_overlap,
                       stringsAsFactors = FALSE)
  return(result)
}
```

With the preallocation, the overlaps of 100 sets is almost instantaneous:

```{r}
system.time(calc_pairwise_overlaps(sets_100))
```

And it can even handle 1000 sets in a reasonable amount of time:

```{r}
system.time(calc_pairwise_overlaps(simulate_sets(n_sets = 1000)))
```

But I had thousands of sets to compare, and this was still slow.

## Check sets once instead of every overlap

After making the above changes, I was stumped as to how to further increase its
speed, so I [posted on RStudio Community][rs-community-post] for ideas. And I
was lucky to get the advice from [Nir Graham][nirgrahamuk] to speed up R's set
functions like `intersect()` by checking the inputs once instead of during every
overlap calculation.

The function `intersect()` converts its inputs to vectors with `as.vector()`
and also removes duplicates by calling `unique()`.

```{r}
intersect
```

Since these calculations are performed during each pairwise overlap, they are
extremely redundant. It is faster to confirm all the input sets are unique
vectors at the very beginning of the function, since then the code no longer
needs to check during each iteration of the loop. When valid input can be
assumed, `intersect()` reduces to:

```{r eval=FALSE}
y[match(x, y, 0L)]
```

It's a similar situation with `union()`. It also converts its inputs to vectors
with `as.vector()`. However, it does require the call to `unique()` to properly
find the union of the two sets.

```{r}
union
```

But even `unique()` itself can be sped up. Since there is no specific `unique()`
method for character vectors (i.e. `unique.character()`), character vectors are
processed with `unique.default()`.

```{r}
methods(unique)
unique.default
```

And the majority of `unique.default()` is code to handle various types of input
like factors, times, or dates. Since I will ensure the input will always be
character vectors, the only required line of `unique.default()` is the one below:

```{r eval=FALSE}
z <- .Internal(unique(x, incomparables, fromLast, nmax))
```

So in this final optimized function, the input sets are first confirmed to be
unique character vectors at the very beginning of the function, and then this
assumption is never tested unnecessarily again.

```{r}
calc_pairwise_overlaps <- function(sets) {
  # Ensure that all sets are unique character vectors
  sets_are_vectors <- vapply(sets, is.vector, logical(1))
  if (any(!sets_are_vectors)) {
    stop("Sets must be vectors")
  }
  sets_are_atomic <- vapply(sets, is.atomic, logical(1))
  if (any(!sets_are_atomic)) {
    stop("Sets must be atomic vectors, i.e. not lists")
  }
  sets <- lapply(sets, as.character)
  is_unique <- function(x) length(unique(x)) == length(x)
  sets_are_unique <- vapply(sets, is_unique, logical(1))
  if (any(!sets_are_unique)) {
    stop("Sets must be unique, i.e. no duplicated elements")
  }

  n_sets <- length(sets)
  set_names <- names(sets)
  n_overlaps <- choose(n = n_sets, k = 2)

  vec_name1 <- character(length = n_overlaps)
  vec_name2 <- character(length = n_overlaps)
  vec_num_shared <- integer(length = n_overlaps)
  vec_overlap <- numeric(length = n_overlaps)
  vec_jaccard <- numeric(length = n_overlaps)
  overlaps_index <- 1

  for (i in seq_len(n_sets - 1)) {
    name1 <- set_names[i]
    set1 <- sets[[i]]
    for (j in seq(i + 1, n_sets)) {
      name2 <- set_names[j]
      set2 <- sets[[j]]

      set_intersect <- set1[match(set2, set1, 0L)]
      set_union <- .Internal(unique(c(set1, set2), incomparables = FALSE,
                                    fromLast = FALSE, nmax = NA))
      num_shared <- length(set_intersect)
      overlap <- num_shared / min(length(set1), length(set2))
      jaccard <- num_shared / length(set_union)

      vec_name1[overlaps_index] <- name1
      vec_name2[overlaps_index] <- name2
      vec_num_shared[overlaps_index] <- num_shared
      vec_overlap[overlaps_index] <- overlap
      vec_jaccard[overlaps_index] <- jaccard

      overlaps_index <- overlaps_index + 1
    }
  }

  result <- data.frame(name1 = vec_name1,
                       name2 = vec_name2,
                       num_shared = vec_num_shared,
                       overlap = vec_overlap,
                       jaccard = vec_overlap,
                       stringsAsFactors = FALSE)
  return(result)
}
```

Again, 100 sets is calculated almost instantaneously.

```{r}
system.time(calc_pairwise_overlaps(sets_100))
```

And even 1000 sets is finished processing in only a few seconds:

```{r}
system.time(calc_pairwise_overlaps(sets_1000))
```

## Conclusions

In this post, I demonstrated how to iteratively increase the speed of a slow
function. The biggest lesson I learned from this experience is that it is
possible to increase the speed of base R functions since they may be designed to
handle multiple different types of inputs.

If you see any ways that this function could be further optimized, please do let
me know!

<details>
  <summary>
    <strong>Click here to view the R session information:</strong>
  </summary>

```{r session-information}
sessionInfo()
```

</details>

[nirgrahamuk]: https://community.rstudio.com/u/nirgrahamuk/
[rs-community-post]: https://community.rstudio.com/t/how-to-efficiently-calculate-pairwise-overlaps-of-many-sets/56750
